{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.regularizers import L1L2\n",
    "#import project modules\n",
    "from modules import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_arr(a, b, *args): \n",
    "    np.random.seed(0)\n",
    "    return np.random.rand(*args) * (b - a) + a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmParam:\n",
    "    def __init__(self, ct_dim, x_dim):\n",
    "        \"\"\"\n",
    "        \n",
    "        initialize all weights and biases \n",
    "        ct_dim - the dimension of current cell state (Ct) matrix\n",
    "        x_dim - the dimension of Tth input\n",
    "        \n",
    "        Weight and bias matrices are initialized with random values instead of zeroes to add noise. \n",
    "        Their derivatives will thus be zero.\n",
    "        \n",
    "        Terminology:\n",
    "        \n",
    "        Prefixes\n",
    "        w - a weight matrix\n",
    "        b - a bias matrix\n",
    "        d - a derivative matrix\n",
    "        \n",
    "        Suffixes\n",
    "        c - cell state gate. Represents data held in current cell. \n",
    "            It is the c't (c bar t) matrix\n",
    "        i - input gate\n",
    "        f - forget gate\n",
    "        o - output gate        \n",
    "        \n",
    "        \"\"\"\n",
    "        self.ct_dim = ct_dim  \n",
    "        self.x_dim = x_dim \n",
    "        combined_dim = x_dim + ct_dim\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.wc = rand_arr(-0.1, 0.1, ct_dim, combined_dim)\n",
    "        self.wi = rand_arr(-0.1, 0.1, ct_dim, combined_dim) \n",
    "        self.wf = rand_arr(-0.1, 0.1, ct_dim, combined_dim)\n",
    "        self.wo = rand_arr(-0.1, 0.1, ct_dim, combined_dim)\n",
    "        \n",
    "        #Initialize biases \n",
    "        self.bc = rand_arr(-0.1, 0.1, ct_dim) \n",
    "        self.bi = rand_arr(-0.1, 0.1, ct_dim) \n",
    "        self.bf = rand_arr(-0.1, 0.1, ct_dim) \n",
    "        self.bo = rand_arr(-0.1, 0.1, ct_dim) \n",
    "        \n",
    "        # Initialize derivatives\n",
    "        self.dwc = np.zeros((ct_dim, combined_dim)) \n",
    "        self.dwi = np.zeros((ct_dim, combined_dim)) \n",
    "        self.dwf = np.zeros((ct_dim, combined_dim)) \n",
    "        self.dwo = np.zeros((ct_dim, combined_dim)) \n",
    "        self.dbc = np.zeros(ct_dim) \n",
    "        self.dbi = np.zeros(ct_dim) \n",
    "        self.dbf = np.zeros(ct_dim) \n",
    "        self.dbo = np.zeros(ct_dim)\n",
    "        \n",
    "    def apply_derivatives(self, alpha = 1):\n",
    "        \"\"\"\n",
    "        Update parameters in each iteration to reach optimal value \n",
    "        alpha is the learning rate.\n",
    "        \"\"\"\n",
    "        self.wc -= alpha * self.dwc\n",
    "        self.wi -= alpha * self.dwi\n",
    "        self.wf -= alpha * self.dwf\n",
    "        self.wo -= alpha * self.dwo\n",
    "        self.bc -= alpha * self.dbc\n",
    "        self.bi -= alpha * self.dbi\n",
    "        self.bf -= alpha * self.dbf\n",
    "        self.bo -= alpha * self.dbo\n",
    "        \n",
    "        # reset all derivatives to zero\n",
    "        self.dwc = np.zeros_like(self.wc)\n",
    "        self.dwi = np.zeros_like(self.wi) \n",
    "        self.dwf = np.zeros_like(self.wf) \n",
    "        self.dwo = np.zeros_like(self.wo) \n",
    "        self.dbc = np.zeros_like(self.bc)\n",
    "        self.dbi = np.zeros_like(self.bi) \n",
    "        self.dbf = np.zeros_like(self.bf) \n",
    "        self.dbo = np.zeros_like(self.bo) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmCellState:\n",
    "    def __init__(self, ct_dim, x_dim):\n",
    "        \"\"\"\n",
    "        Initialize all gate matrices. \n",
    "        All gate matrices have the same dimension as ct matrix\n",
    "        c - current hidden cell state. The gate corresponding to this determines how much \n",
    "            data of previous cell should be read in current cell.\n",
    "        i - input gate. Determines how much data should be read into the cell from current input.\n",
    "        f - forget gate. Determines how much data should be forgotten, i.e discarded\n",
    "        o - output gate. How much data to output from current cell to next cell\n",
    "        s - The present state of gate. \n",
    "            Equation to calculate present state : forget_gate*previous_state(s<t-1>) + c_gate*input_gate\n",
    "        h - output state of the cell. It is the prediction value of Tth output in series.\n",
    "        dh - derivative of h\n",
    "        ds - derivative of s\n",
    "        \"\"\"\n",
    "        self.c = np.zeros(ct_dim)\n",
    "        self.i = np.zeros(ct_dim)\n",
    "        self.f = np.zeros(ct_dim)\n",
    "        self.o = np.zeros(ct_dim)\n",
    "        self.s = np.zeros(ct_dim)\n",
    "        self.h = np.zeros(ct_dim)\n",
    "        self.dh = np.zeros_like(self.h)\n",
    "        self.ds = np.zeros_like(self.s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmLayer:\n",
    "    def __init__(self, lstm_param, lstm_cell_state):\n",
    "        self.state = lstm_cell_state\n",
    "        self.param = lstm_param\n",
    "        #xh is the concatenation ofprevious layer's output with current input.\n",
    "        self.xh = None\n",
    "\n",
    "    def calculate_gates(self, x, s_prev = None, h_prev = None):\n",
    "        \"\"\"\n",
    "        if this is the first lstm layer in the network then \n",
    "        s_prev and h_prev will be initialized to zero as cell state \n",
    "        and output states are not present.\n",
    "        \n",
    "        s_prev = cell state of previous layer's cells.\n",
    "        h_prev = output state from previous layer\n",
    "        \n",
    "        \"\"\"\n",
    "        if s_prev is None: \n",
    "            s_prev = np.zeros_like(self.state.s)\n",
    "        if h_prev is None: \n",
    "            h_prev = np.zeros_like(self.state.h)\n",
    "        \n",
    "        # save previous states for use in backprop\n",
    "        self.s_prev = s_prev\n",
    "        self.h_prev = h_prev\n",
    "\n",
    "        # concatenate x(T) and h(T-1)\n",
    "        xh = np.hstack((x,  h_prev))\n",
    "        #Calculate gate values\n",
    "        self.state.c = tanh(np.dot(self.param.wc, xh) + self.param.bc)\n",
    "        self.state.i = sigmoid(np.dot(self.param.wi, xh) + self.param.bi)\n",
    "        self.state.f = sigmoid(np.dot(self.param.wf, xh) + self.param.bf)\n",
    "        self.state.o = sigmoid(np.dot(self.param.wo, xh) + self.param.bo)\n",
    "        self.state.s = self.state.c * self.state.i + s_prev * self.state.f\n",
    "        self.state.h = self.state.s * self.state.o\n",
    "        self.xh = xh\n",
    "    \n",
    "    def calculate_derivatives(self, dh, ds):\n",
    "        ds = self.state.o * dh + ds\n",
    "        do = self.state.s * dh\n",
    "        di = self.state.c * ds\n",
    "        dc = self.state.i * ds\n",
    "        df = self.s_prev * ds\n",
    "\n",
    "        # calculate derivatives w.r.t. gate inside sigma / tanh function\n",
    "        di_input = dsigmoid(self.state.i) * di \n",
    "        df_input = dsigmoid(self.state.f) * df \n",
    "        do_input = dsigmoid(self.state.o) * do \n",
    "        dc_input = dtanh(self.state.c) * dc\n",
    "\n",
    "        # derivatives w.r.t. inputs\n",
    "        self.param.dwc += np.outer(dc_input, self.xh)\n",
    "        self.param.dwi += np.outer(di_input, self.xh)\n",
    "        self.param.dwf += np.outer(df_input, self.xh)\n",
    "        self.param.dwo += np.outer(do_input, self.xh)\n",
    "        self.param.dbc += dc_input       \n",
    "        self.param.dbi += di_input\n",
    "        self.param.dbf += df_input       \n",
    "        self.param.dbo += do_input\n",
    "        \n",
    "        # calculate derivative for xh\n",
    "        dxh = np.zeros_like(self.xh)\n",
    "        dxh += np.dot(self.param.wc.T, dc_input)\n",
    "        dxh += np.dot(self.param.wi.T, di_input)\n",
    "        dxh += np.dot(self.param.wf.T, df_input)\n",
    "        dxh += np.dot(self.param.wo.T, do_input)\n",
    "        \n",
    "        # save derivatives\n",
    "        self.state.ds = ds * self.state.f\n",
    "        self.state.dh = dxh[self.param.x_dim:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmNetwork():\n",
    "    def __init__(self, lstm_param):\n",
    "        \"\"\"\n",
    "        x_list - the sequence that'll be iput to lstm\n",
    "        lstm_layer_list - the ouput from layer that will be input to next layer\n",
    "        \"\"\"\n",
    "        self.lstm_param = lstm_param\n",
    "        self.lstm_layer_list = []\n",
    "        self.x_list = []\n",
    "\n",
    "    def get_loss(self, y_list, loss_layer):\n",
    "        \"\"\"\n",
    "        Updates derivatives w.r.t corresponding loss layer. \n",
    "        To update parameters, we will call self.lstm_param.apply_derivatives()\n",
    "        \"\"\"\n",
    "        index = len(self.x_list) - 1\n",
    "        # Calculate loss for the last layer \n",
    "        loss = loss_layer.loss(self.lstm_layer_list[index].state.h, y_list[index])\n",
    "        diff_h = loss_layer.derivative(self.lstm_layer_list[index].state.h, y_list[index])\n",
    "        # For the last layer of the network, diff_s will be\n",
    "        diff_s = np.zeros(self.lstm_param.ct_dim)\n",
    "        self.lstm_layer_list[index].calculate_derivatives(diff_h, diff_s)\n",
    "        index -= 1\n",
    "\n",
    "        while index >= 0:\n",
    "            loss += loss_layer.loss(self.lstm_layer_list[index].state.h, y_list[index])\n",
    "            diff_h = loss_layer.derivative(self.lstm_layer_list[index].state.h, y_list[index])\n",
    "            diff_h += self.lstm_layer_list[index + 1].state.dh\n",
    "            diff_s = self.lstm_layer_list[index + 1].state.ds\n",
    "            self.lstm_layer_list[index].calculate_derivatives(diff_h, diff_s)\n",
    "            index -= 1 \n",
    "\n",
    "        return loss\n",
    "\n",
    "    def clear_x_list(self):\n",
    "        self.x_list = []\n",
    "\n",
    "    def add_x_list(self, x):\n",
    "        self.x_list.append(x)\n",
    "        if len(self.x_list) > len(self.lstm_layer_list):\n",
    "            lstm_state = LstmCellState(self.lstm_param.ct_dim, self.lstm_param.x_dim)\n",
    "            self.lstm_layer_list.append(LstmLayer(self.lstm_param, lstm_state))\n",
    "\n",
    "        # get index of most recent x input\n",
    "        index = len(self.x_list) - 1\n",
    "        if index == 0:\n",
    "            self.lstm_layer_list[index].calculate_gates(x)\n",
    "        else:\n",
    "            s_prev = self.lstm_layer_list[index - 1].state.s\n",
    "            h_prev = self.lstm_layer_list[index - 1].state.h\n",
    "            self.lstm_layer_list[index].calculate_gates(x, s_prev, h_prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dataset(stock_market_dataset):\n",
    "    normalizer = preprocessing.MinMaxScaler((0,1))\n",
    "    smd_exclude_strings = stock_market_dataset.select_dtypes(include = [np.number])\n",
    "    normalized = normalizer.fit_transform(smd_exclude_strings)\n",
    "    return [normalized, normalizer]\n",
    "\n",
    "def denormalize_data(df, normalizer, time_step, predictions, close_value_col_index):\n",
    "    df_copy = df.copy(deep = True)\n",
    "    rows_to_drop = [i for i in range(0,time_step)]\n",
    "    df_copy.drop(rows_to_drop,inplace = True)     \n",
    "    #deleting first #time_step rows of normalized_df. Because output prediction for the first 30 days (first time slice) doesn't exist.\n",
    "\n",
    "    #converting prediction array to dataframe for replacing actual row with prediction row\n",
    "    predicted_normalised_values_df = pd.DataFrame(predictions) \n",
    "\n",
    "    #replacing\n",
    "    df_copy[close_value_col_index][0:len(predictions)] = predicted_normalised_values_df[0].values\n",
    "    \n",
    "    temp = normalizer.inverse_transform(df_copy)\n",
    "    df_copy = pd.DataFrame(temp)\n",
    "    return df_copy[close_value_col_index][0:len(predictions)]\n",
    "\n",
    "def make_train_test_val_sets(dataarray, num_features, close_value_col_index, time_step, train_per, val_per, test_per):\n",
    "    n = dataarray.shape[0] \n",
    "\n",
    "    #slices of data into time_steps \n",
    "    X_slice=[]\n",
    "    y_slice=[]\n",
    "    #normalized df dim 3125, 13\n",
    "    # i = 30 - 3125\n",
    "\n",
    "    for i in range(time_step, n):\n",
    "        X_slice.append(dataarray[ i-time_step:i ,  close_value_col_index] )      #1 example having data from 30 days dim 30x13\n",
    "        y_slice.append(dataarray[i,close_value_col_index])                  #close value data of the 31st day \n",
    "\n",
    "    #splitting percentage\n",
    "\n",
    "    #splitting slices for test,val,train and converting into np array\n",
    "    X_train=np.array( X_slice[ 0:int(n*train_per) ])\n",
    "    y_train=np.array( y_slice[ 0:int(n*train_per) ])\n",
    "\n",
    "    X_val=np.array( X_slice[ int(n*train_per):int(n*(train_per+val_per)) ])\n",
    "    y_val=np.array( y_slice[ int(n*train_per):int(n*(train_per+val_per)) ])\n",
    "\n",
    "    X_test=np.array( X_slice[ int(n*(train_per+val_per)):int(n*(train_per+val_per+test_per))])\n",
    "    y_test=np.array( y_slice[ int(n*(train_per+val_per)):int(n*(train_per+val_per+test_per))])\n",
    "    \n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], num_features))    \n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], X_val.shape[1], num_features))    \n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], num_features))    \n",
    "    #print(np.array(X_train).shape)\n",
    "    #X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], num_features))    \n",
    "    #dimension of X_train 3095, 30, 13\n",
    "    #dimension of y_train  \n",
    "    #print(np.array(X_train).shape)\n",
    "    return [X_train, y_train, X_val, y_val, X_test, y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Dividends</th>\n",
       "      <th>Stock Splits</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2009-12-31</th>\n",
       "      <td>24.073285</td>\n",
       "      <td>24.081056</td>\n",
       "      <td>23.684755</td>\n",
       "      <td>23.684755</td>\n",
       "      <td>31929700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-04</th>\n",
       "      <td>23.793539</td>\n",
       "      <td>24.166527</td>\n",
       "      <td>23.770227</td>\n",
       "      <td>24.049969</td>\n",
       "      <td>38409100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-05</th>\n",
       "      <td>23.972268</td>\n",
       "      <td>24.166532</td>\n",
       "      <td>23.809084</td>\n",
       "      <td>24.057743</td>\n",
       "      <td>49749600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-06</th>\n",
       "      <td>23.995573</td>\n",
       "      <td>24.150985</td>\n",
       "      <td>23.715832</td>\n",
       "      <td>23.910097</td>\n",
       "      <td>58182400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-07</th>\n",
       "      <td>23.801302</td>\n",
       "      <td>23.855697</td>\n",
       "      <td>23.459397</td>\n",
       "      <td>23.661432</td>\n",
       "      <td>50559700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-17</th>\n",
       "      <td>165.284340</td>\n",
       "      <td>165.333705</td>\n",
       "      <td>163.319719</td>\n",
       "      <td>164.968430</td>\n",
       "      <td>34371700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-21</th>\n",
       "      <td>164.553765</td>\n",
       "      <td>166.044512</td>\n",
       "      <td>164.306954</td>\n",
       "      <td>164.376068</td>\n",
       "      <td>29517200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-22</th>\n",
       "      <td>165.264584</td>\n",
       "      <td>165.353448</td>\n",
       "      <td>163.566524</td>\n",
       "      <td>163.586273</td>\n",
       "      <td>24138800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-23</th>\n",
       "      <td>164.070024</td>\n",
       "      <td>164.672243</td>\n",
       "      <td>163.161761</td>\n",
       "      <td>164.593262</td>\n",
       "      <td>19680800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-24</th>\n",
       "      <td>165.373186</td>\n",
       "      <td>165.392935</td>\n",
       "      <td>162.352222</td>\n",
       "      <td>162.934692</td>\n",
       "      <td>24918100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2533 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Open        High         Low       Close    Volume  \\\n",
       "Date                                                                   \n",
       "2009-12-31   24.073285   24.081056   23.684755   23.684755  31929700   \n",
       "2010-01-04   23.793539   24.166527   23.770227   24.049969  38409100   \n",
       "2010-01-05   23.972268   24.166532   23.809084   24.057743  49749600   \n",
       "2010-01-06   23.995573   24.150985   23.715832   23.910097  58182400   \n",
       "2010-01-07   23.801302   23.855697   23.459397   23.661432  50559700   \n",
       "...                ...         ...         ...         ...       ...   \n",
       "2020-01-17  165.284340  165.333705  163.319719  164.968430  34371700   \n",
       "2020-01-21  164.553765  166.044512  164.306954  164.376068  29517200   \n",
       "2020-01-22  165.264584  165.353448  163.566524  163.586273  24138800   \n",
       "2020-01-23  164.070024  164.672243  163.161761  164.593262  19680800   \n",
       "2020-01-24  165.373186  165.392935  162.352222  162.934692  24918100   \n",
       "\n",
       "            Dividends  Stock Splits  \n",
       "Date                                 \n",
       "2009-12-31        0.0             0  \n",
       "2010-01-04        0.0             0  \n",
       "2010-01-05        0.0             0  \n",
       "2010-01-06        0.0             0  \n",
       "2010-01-07        0.0             0  \n",
       "...               ...           ...  \n",
       "2020-01-17        0.0             0  \n",
       "2020-01-21        0.0             0  \n",
       "2020-01-22        0.0             0  \n",
       "2020-01-23        0.0             0  \n",
       "2020-01-24        0.0             0  \n",
       "\n",
       "[2533 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tickerSymbol = 'MSFT'\n",
    "tickerData = yf.Ticker(tickerSymbol)\n",
    "stock_market_dataset = tickerData.history(period='1d', start='2010-1-1', end='2020-1-25')\n",
    "stock_market_dataset\n",
    "#data_set_file = 'Google.csv'\n",
    "#stock_market_dataset = pd.read_csv(data_set_file)\n",
    "#stock_market_dataset.shape\n",
    "#stock_market_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37, 30, 1) training dataset shape\n",
      "(38, 30, 1)  val dataset shape\n",
      "(9, 30, 1)  test dataset shape\n"
     ]
    }
   ],
   "source": [
    "###split data set into train, cross validation and test set\n",
    "\n",
    "num_features = 6 \n",
    "close_value_col_index = 4\n",
    "select_cols = stock_market_dataset[['Close']]\n",
    "new_df = select_cols.copy()\n",
    "\n",
    "###normalize data\n",
    "normalized_array, normalizer = normalize_dataset(new_df)\n",
    "#normalized_df.head()\n",
    "#n = normalized_df.shape[0]\n",
    "\n",
    "#creating training set with time steps.\n",
    "\n",
    "time_step=30    #1 month time step\n",
    "train_per = 0.015\n",
    "val_per = 0.015\n",
    "test_per = 0.0035\n",
    "    \n",
    "X_train, y_train, X_val, y_val, X_test, y_test =  make_train_test_val_sets(normalized_array, 1, 0, time_step, train_per, val_per, test_per)\n",
    "\n",
    "print(str(X_train.shape)+ \" training dataset shape\")\n",
    "print(str(X_val.shape)+ \"  val dataset shape\")\n",
    "print(str(X_test.shape)+ \"  test dataset shape\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LstmParam' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-9663e4215524>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mmem_cell_ct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mx_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mlstm_param\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLstmParam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmem_cell_ct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mlstm_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLstmNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_param\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0my_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LstmParam' is not defined"
     ]
    }
   ],
   "source": [
    "class LossLayer:\n",
    "    \"\"\"\n",
    "    Computes square loss with first element of hidden layer array.\n",
    "    \"\"\"\n",
    "    @classmethod\n",
    "    def loss(self, pred, label):\n",
    "        return (pred[0] - label) ** 2\n",
    "\n",
    "    @classmethod\n",
    "    def derivative(self, pred, label):\n",
    "        diff = np.zeros_like(pred)\n",
    "        diff[0] = 2 * (pred[0] - label)\n",
    "        return diff\n",
    "    \n",
    "    def percentage_error(pred, label):\n",
    "        return (100/len(pred))*np.sum(np.absolute(np.array(label)-np.array(pred))/np.array(label))\n",
    "\n",
    "\n",
    "# learns to repeat simple sequence from random inputs\n",
    "np.random.seed(0)\n",
    "\n",
    "# parameters for input data dimension and lstm cell count\n",
    "mem_cell_ct = 100\n",
    "x_dim = time_step\n",
    "lstm_param = LstmParam(mem_cell_ct, x_dim)\n",
    "lstm_net = LstmNetwork(lstm_param)\n",
    "y_list = y_train.tolist()\n",
    "input_val_arr = X_train.tolist()\n",
    "for cur_iter in range(1200):\n",
    "    if ((cur_iter+1)%100 == 0):\n",
    "        print(\"iter\", \"%2s\" % str(cur_iter+1), end=\": \")\n",
    "    for ind in range(len(y_list)):\n",
    "        lstm_net.add_x_list(input_val_arr[ind])\n",
    "\n",
    "    #print(\"y_pred = [\" +\n",
    "    #      \", \".join(\"[% 2.5f % 2.5f]\" % (lstm_net.lstm_layer_list[ind].state.h[0], y_list[ind]) for ind in range(len(y_list))) +\n",
    "    #      \"]\", end=\", \")\n",
    "\n",
    "    loss = lstm_net.get_loss(y_list, LossLayer)\n",
    "    if ((cur_iter+1)%100 == 0):\n",
    "        print(\"loss:\", \"%.3e\" % loss)\n",
    "    lstm_param.apply_derivatives(0.1)\n",
    "    lstm_net.clear_x_list()\n",
    "\n",
    "pred = []\n",
    "for ind in range(len(y_list)):\n",
    "    pred.append(lstm_net.lstm_layer_list[ind].state.h[0])\n",
    "\n",
    "print(\"Accuracy: % 2.2f%%\" % (100-LossLayer.percentage_error(pred, y_list)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross Val \n",
    "y_list = y_val.tolist()\n",
    "input_val_arr = X_val.tolist()\n",
    "for ind in range(len(y_list)):\n",
    "    lstm_net.add_x_list(input_val_arr[ind])\n",
    "\n",
    "pred = []\n",
    "for ind in range(len(y_list)):\n",
    "    pred.append(lstm_net.lstm_layer_list[ind].state.h[0])\n",
    "\n",
    "print(len(pred))\n",
    "lstm_net.clear_x_list()\n",
    "\n",
    "print(\"Accuracy: % 2.2f%%\" % (100-LossLayer.percentage_error(pred, y_list)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test  \n",
    "y_list = y_test.tolist()\n",
    "input_val_arr = X_test.tolist()\n",
    "for ind in range(len(y_list)):\n",
    "    lstm_net.add_x_list(input_val_arr[ind])\n",
    "\n",
    "pred = []\n",
    "for ind in range(len(y_list)):\n",
    "    pred.append(lstm_net.lstm_layer_list[ind].state.h[0])\n",
    "\n",
    "print(len(pred))\n",
    "lstm_net.clear_x_list()\n",
    "print(\"Accuracy: % 2.2f%%\" % (100-LossLayer.percentage_error(pred, y_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denormed = denormalize_data(pd.DataFrame(normalized_array), normalizer, time_step, pred, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(df, day_index, time_step, lstm_net, y_list):   #date should be greater than time_step\n",
    "    actual = df['Close'][day_index]    \n",
    "    select_cols = df[['Close']]\n",
    "    new_df = select_cols.copy()\n",
    "    normalized, normalizer = normalize_dataset(new_df)\n",
    "    norm_actual = normalized[day_index][0]\n",
    "    y_list.append(norm_actual)\n",
    "    sequence = [normalized[i] for i in range(day_index-time_step,day_index)]\n",
    "    lstm_net.add_x_list(np.reshape(np.array(sequence), (time_step)).tolist())\n",
    "    pred = lstm_net.lstm_layer_list[len(lstm_net.lstm_layer_list)-1].state.h[0]\n",
    "    normalized[day_index][0] = pred\n",
    "    temp = normalizer.inverse_transform(pd.DataFrame(normalized))\n",
    "    lstm_net.clear_x_list()\n",
    "    #print(temp[day_index][0], actual)\n",
    "    return temp[day_index][0], actual\n",
    "    \n",
    "\n",
    "get_prediction(stock_market_dataset, 50, 30, lstm_net, y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1773, 30, 1) training dataset shape\n",
      "(506, 30, 1)  val dataset shape\n",
      "(224, 30, 1)  test dataset shape\n",
      "178/178 [==============================] - 36s 147ms/step - loss: 0.0114\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7fee6e1fa0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prediction With keras\n",
    "train_per = 0.7\n",
    "val_per = 0.2\n",
    "test_per = 0.1\n",
    "X_train, y_train, X_val, y_val, X_test, y_test =  make_train_test_val_sets(normalized_array, 1, 0, time_step, train_per, val_per, test_per)\n",
    "\n",
    "print(str(X_train.shape)+ \" training dataset shape\")\n",
    "print(str(X_val.shape)+ \"  val dataset shape\")\n",
    "print(str(X_test.shape)+ \"  test dataset shape\")\n",
    "#Implementing LSTM\n",
    "model = keras.models.Sequential()         #initializing network\n",
    "\n",
    "#input layer\n",
    "hidden_layer_units = 10\n",
    "model.add(keras.layers.LSTM(hidden_layer_units,kernel_regularizer=L1L2(0.0001), return_sequences=True, input_shape=(X_train.shape[1],1)))\n",
    "\n",
    "#LSTM layer 2\n",
    "hidden_layer_units = 10\n",
    "model.add(keras.layers.LSTM(hidden_layer_units,kernel_regularizer=L1L2(0.0001)))\n",
    "\n",
    "#Output layer\n",
    "output_layer_units = 1     #just need the close value\n",
    "model.add(keras.layers.Dense(output_layer_units))\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "model.fit(X_train, y_train, epochs = 70, batch_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2503, 0)\n",
      "(224, 1) (224, 30, 1)\n"
     ]
    }
   ],
   "source": [
    "real_close_value = pd.DataFrame(normalized_array).iloc[ time_step: , close_value_col_index:close_value_col_index+1].values     #Close column normalized real value. Ideally predictions should be on test set.\n",
    "\n",
    "print(real_close_value.shape)\n",
    "\n",
    "predicted_close_value_train = model.predict(X_train)\n",
    "predicted_close_value_val = model.predict(X_val)\n",
    "predicted_close_value_test  = model.predict(X_test)   \n",
    "combined_prediction = np.append(predicted_close_value_train,np.append(predicted_close_value_val,predicted_close_value_test))\n",
    "\n",
    "print(predicted_close_value_test.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2503, 1) (2503, 1)\n"
     ]
    }
   ],
   "source": [
    "temp = normalizer.inverse_transform(pd.DataFrame(normalized_array))\n",
    "real_close_value = pd.DataFrame(temp)\n",
    "temp = normalizer.inverse_transform(pd.DataFrame(combined_prediction))\n",
    "predicted_close_value = pd.DataFrame(temp)\n",
    "\n",
    "rows_to_drop = [i for i in range(0,time_step)]\n",
    "real_close_value.drop(rows_to_drop,inplace = True)     #deleting first 30 rows of normalized_df. Because output prediction for the first 30 days (first time slice) doesn't exist.\n",
    "print(predicted_close_value.shape, real_close_value.shape)\n",
    "real_close_value = np.array(real_close_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30,)\n",
      "[[33.989742]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2503, 1)\n"
     ]
    }
   ],
   "source": [
    "def get_pred(day_index, model, label, time_step, num_features, normalizer):\n",
    "    if day_index>len(label):\n",
    "        print(\"Day:\",day_index,\"is out of series range:\",len(label))\n",
    "        return\n",
    "    \n",
    "    sequence = np.array([label[i][0] for i in range(day_index-time_step,day_index)])\n",
    "    print(sequence.shape)\n",
    "    sequence = np.reshape(sequence, (1, time_step, num_features))\n",
    "    pred = model.predict(sequence)\n",
    "    pred = normalizer.inverse_transform(pd.DataFrame(pred))\n",
    "    x_coords = [day_index-1, day_index]\n",
    "    y_coords = [label[day_index-1][0], pred[0][0]]\n",
    "    \n",
    "    plt.plot(x_coords, y_coords, color = 'blue', label = 'Close')\n",
    "    plt.xticks(np.arange(day_index-1, day_index+1, 1))\n",
    "    plt.title('Previous Day vs Today')\n",
    "    plt.ylabel('Day')\n",
    "    plt.ylabel('Close')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "get_pred(200, model, real_close_value, time_step, 1, normalizer)\n",
    "print(real_close_value.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lstmenv",
   "language": "python",
   "name": "lstmenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
