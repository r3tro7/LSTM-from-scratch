{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.regularizers import L1L2\n",
    "#import project modules\n",
    "from modules import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_arr(a, b, *args): \n",
    "    np.random.seed(0)\n",
    "    return np.random.rand(*args) * (b - a) + a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmParam:\n",
    "    def __init__(self, ct_dim, x_dim):\n",
    "        \"\"\"\n",
    "        \n",
    "        initialize all weights and biases \n",
    "        ct_dim - the dimension of current cell state (Ct) matrix\n",
    "        x_dim - the dimension of Tth input\n",
    "        \n",
    "        Weight and bias matrices are initialized with random values instead of zeroes to add noise. \n",
    "        Their derivatives will thus be zero.\n",
    "        \n",
    "        Terminology:\n",
    "        \n",
    "        Prefixes\n",
    "        w - a weight matrix\n",
    "        b - a bias matrix\n",
    "        d - a derivative matrix\n",
    "        \n",
    "        Suffixes\n",
    "        c - cell state gate. Represents data held in current cell. \n",
    "            It is the c't (c bar t) matrix\n",
    "        i - input gate\n",
    "        f - forget gate\n",
    "        o - output gate        \n",
    "        \n",
    "        \"\"\"\n",
    "        self.ct_dim = ct_dim  \n",
    "        self.x_dim = x_dim \n",
    "        combined_dim = x_dim + ct_dim\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.wc = rand_arr(-0.1, 0.1, ct_dim, combined_dim)\n",
    "        self.wi = rand_arr(-0.1, 0.1, ct_dim, combined_dim) \n",
    "        self.wf = rand_arr(-0.1, 0.1, ct_dim, combined_dim)\n",
    "        self.wo = rand_arr(-0.1, 0.1, ct_dim, combined_dim)\n",
    "        \n",
    "        #Initialize biases \n",
    "        self.bc = rand_arr(-0.1, 0.1, ct_dim) \n",
    "        self.bi = rand_arr(-0.1, 0.1, ct_dim) \n",
    "        self.bf = rand_arr(-0.1, 0.1, ct_dim) \n",
    "        self.bo = rand_arr(-0.1, 0.1, ct_dim) \n",
    "        \n",
    "        # Initialize derivatives\n",
    "        self.dwc = np.zeros((ct_dim, combined_dim)) \n",
    "        self.dwi = np.zeros((ct_dim, combined_dim)) \n",
    "        self.dwf = np.zeros((ct_dim, combined_dim)) \n",
    "        self.dwo = np.zeros((ct_dim, combined_dim)) \n",
    "        self.dbc = np.zeros(ct_dim) \n",
    "        self.dbi = np.zeros(ct_dim) \n",
    "        self.dbf = np.zeros(ct_dim) \n",
    "        self.dbo = np.zeros(ct_dim)\n",
    "        \n",
    "    def apply_derivatives(self, alpha = 1):\n",
    "        \"\"\"\n",
    "        Update parameters in each iteration to reach optimal value \n",
    "        alpha is the learning rate.\n",
    "        \"\"\"\n",
    "        self.wc -= alpha * self.dwc\n",
    "        self.wi -= alpha * self.dwi\n",
    "        self.wf -= alpha * self.dwf\n",
    "        self.wo -= alpha * self.dwo\n",
    "        self.bc -= alpha * self.dbc\n",
    "        self.bi -= alpha * self.dbi\n",
    "        self.bf -= alpha * self.dbf\n",
    "        self.bo -= alpha * self.dbo\n",
    "        \n",
    "        # reset all derivatives to zero\n",
    "        self.dwc = np.zeros_like(self.wc)\n",
    "        self.dwi = np.zeros_like(self.wi) \n",
    "        self.dwf = np.zeros_like(self.wf) \n",
    "        self.dwo = np.zeros_like(self.wo) \n",
    "        self.dbc = np.zeros_like(self.bc)\n",
    "        self.dbi = np.zeros_like(self.bi) \n",
    "        self.dbf = np.zeros_like(self.bf) \n",
    "        self.dbo = np.zeros_like(self.bo) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmCellState:\n",
    "    def __init__(self, ct_dim, x_dim):\n",
    "        \"\"\"\n",
    "        Initialize all gate matrices. \n",
    "        All gate matrices have the same dimension as ct matrix\n",
    "        c - current hidden cell state. The gate corresponding to this determines how much \n",
    "            data of previous cell should be read in current cell.\n",
    "        i - input gate. Determines how much data should be read into the cell from current input.\n",
    "        f - forget gate. Determines how much data should be forgotten, i.e discarded\n",
    "        o - output gate. How much data to output from current cell to next cell\n",
    "        s - The present state of gate. \n",
    "            Equation to calculate present state : forget_gate*previous_state(s<t-1>) + c_gate*input_gate\n",
    "        h - output state of the cell. It is the prediction value of Tth output in series.\n",
    "        dh - derivative of h\n",
    "        ds - derivative of s\n",
    "        \"\"\"\n",
    "        self.c = np.zeros(ct_dim)\n",
    "        self.i = np.zeros(ct_dim)\n",
    "        self.f = np.zeros(ct_dim)\n",
    "        self.o = np.zeros(ct_dim)\n",
    "        self.s = np.zeros(ct_dim)\n",
    "        self.h = np.zeros(ct_dim)\n",
    "        self.dh = np.zeros_like(self.h)\n",
    "        self.ds = np.zeros_like(self.s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmLayer:\n",
    "    def __init__(self, lstm_param, lstm_cell_state):\n",
    "        self.state = lstm_cell_state\n",
    "        self.param = lstm_param\n",
    "        #xh is the concatenation ofprevious layer's output with current input.\n",
    "        self.xh = None\n",
    "\n",
    "    def calculate_gates(self, x, s_prev = None, h_prev = None):\n",
    "        \"\"\"\n",
    "        if this is the first lstm layer in the network then \n",
    "        s_prev and h_prev will be initialized to zero as cell state \n",
    "        and output states are not present.\n",
    "        \n",
    "        s_prev = cell state of previous layer's cells.\n",
    "        h_prev = output state from previous layer\n",
    "        \n",
    "        \"\"\"\n",
    "        if s_prev is None: \n",
    "            s_prev = np.zeros_like(self.state.s)\n",
    "        if h_prev is None: \n",
    "            h_prev = np.zeros_like(self.state.h)\n",
    "        \n",
    "        # save previous states for use in backprop\n",
    "        self.s_prev = s_prev\n",
    "        self.h_prev = h_prev\n",
    "\n",
    "        # concatenate x(T) and h(T-1)\n",
    "        xh = np.hstack((x,  h_prev))\n",
    "        #Calculate gate values\n",
    "        self.state.c = tanh(np.dot(self.param.wc, xh) + self.param.bc)\n",
    "        self.state.i = sigmoid(np.dot(self.param.wi, xh) + self.param.bi)\n",
    "        self.state.f = sigmoid(np.dot(self.param.wf, xh) + self.param.bf)\n",
    "        self.state.o = sigmoid(np.dot(self.param.wo, xh) + self.param.bo)\n",
    "        self.state.s = self.state.c * self.state.i + s_prev * self.state.f\n",
    "        self.state.h = self.state.s * self.state.o\n",
    "        self.xh = xh\n",
    "    \n",
    "    def calculate_derivatives(self, dh, ds):\n",
    "        ds = self.state.o * dh + ds\n",
    "        do = self.state.s * dh\n",
    "        di = self.state.c * ds\n",
    "        dc = self.state.i * ds\n",
    "        df = self.s_prev * ds\n",
    "\n",
    "        # calculate derivatives w.r.t. gate inside sigma / tanh function\n",
    "        di_input = dsigmoid(self.state.i) * di \n",
    "        df_input = dsigmoid(self.state.f) * df \n",
    "        do_input = dsigmoid(self.state.o) * do \n",
    "        dc_input = dtanh(self.state.c) * dc\n",
    "\n",
    "        # derivatives w.r.t. inputs\n",
    "        self.param.dwc += np.outer(dc_input, self.xh)\n",
    "        self.param.dwi += np.outer(di_input, self.xh)\n",
    "        self.param.dwf += np.outer(df_input, self.xh)\n",
    "        self.param.dwo += np.outer(do_input, self.xh)\n",
    "        self.param.dbc += dc_input       \n",
    "        self.param.dbi += di_input\n",
    "        self.param.dbf += df_input       \n",
    "        self.param.dbo += do_input\n",
    "        \n",
    "        # calculate derivative for xh\n",
    "        dxh = np.zeros_like(self.xh)\n",
    "        dxh += np.dot(self.param.wc.T, dc_input)\n",
    "        dxh += np.dot(self.param.wi.T, di_input)\n",
    "        dxh += np.dot(self.param.wf.T, df_input)\n",
    "        dxh += np.dot(self.param.wo.T, do_input)\n",
    "        \n",
    "        # save derivatives\n",
    "        self.state.ds = ds * self.state.f\n",
    "        self.state.dh = dxh[self.param.x_dim:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmNetwork():\n",
    "    def __init__(self, lstm_param):\n",
    "        \"\"\"\n",
    "        x_list - the sequence that'll be iput to lstm\n",
    "        lstm_layer_list - the ouput from layer that will be input to next layer\n",
    "        \"\"\"\n",
    "        self.lstm_param = lstm_param\n",
    "        self.lstm_layer_list = []\n",
    "        self.x_list = []\n",
    "\n",
    "    def get_loss(self, y_list, loss_layer):\n",
    "        \"\"\"\n",
    "        Updates derivatives w.r.t corresponding loss layer. \n",
    "        To update parameters, we will call self.lstm_param.apply_derivatives()\n",
    "        \"\"\"\n",
    "        index = len(self.x_list) - 1\n",
    "        # Calculate loss for the last layer \n",
    "        loss = loss_layer.loss(self.lstm_layer_list[index].state.h, y_list[index])\n",
    "        diff_h = loss_layer.derivative(self.lstm_layer_list[index].state.h, y_list[index])\n",
    "        # For the last layer of the network, diff_s will be\n",
    "        diff_s = np.zeros(self.lstm_param.ct_dim)\n",
    "        self.lstm_layer_list[index].calculate_derivatives(diff_h, diff_s)\n",
    "        index -= 1\n",
    "\n",
    "        while index >= 0:\n",
    "            loss += loss_layer.loss(self.lstm_layer_list[index].state.h, y_list[index])\n",
    "            diff_h = loss_layer.derivative(self.lstm_layer_list[index].state.h, y_list[index])\n",
    "            diff_h += self.lstm_layer_list[index + 1].state.dh\n",
    "            diff_s = self.lstm_layer_list[index + 1].state.ds\n",
    "            self.lstm_layer_list[index].calculate_derivatives(diff_h, diff_s)\n",
    "            index -= 1 \n",
    "\n",
    "        return loss\n",
    "\n",
    "    def clear_x_list(self):\n",
    "        self.x_list = []\n",
    "\n",
    "    def add_x_list(self, x):\n",
    "        self.x_list.append(x)\n",
    "        if len(self.x_list) > len(self.lstm_layer_list):\n",
    "            lstm_state = LstmCellState(self.lstm_param.ct_dim, self.lstm_param.x_dim)\n",
    "            self.lstm_layer_list.append(LstmLayer(self.lstm_param, lstm_state))\n",
    "\n",
    "        # get index of most recent x input\n",
    "        index = len(self.x_list) - 1\n",
    "        if index == 0:\n",
    "            self.lstm_layer_list[index].calculate_gates(x)\n",
    "        else:\n",
    "            s_prev = self.lstm_layer_list[index - 1].state.s\n",
    "            h_prev = self.lstm_layer_list[index - 1].state.h\n",
    "            self.lstm_layer_list[index].calculate_gates(x, s_prev, h_prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dataset(stock_market_dataset):\n",
    "    normalizer = preprocessing.MinMaxScaler((0,1))\n",
    "    smd_exclude_strings = stock_market_dataset.select_dtypes(include = [np.number])\n",
    "    normalized = normalizer.fit_transform(smd_exclude_strings)\n",
    "    return [normalized, normalizer]\n",
    "\n",
    "def denormalize_data(df, normalizer, time_step, predictions, close_value_col_index):\n",
    "    df_copy = df.copy(deep = True)\n",
    "    rows_to_drop = [i for i in range(0,time_step)]\n",
    "    df_copy.drop(rows_to_drop,inplace = True)     \n",
    "    #deleting first #time_step rows of normalized_df. Because output prediction for the first 30 days (first time slice) doesn't exist.\n",
    "\n",
    "    #converting prediction array to dataframe for replacing actual row with prediction row\n",
    "    predicted_normalised_values_df = pd.DataFrame(predictions) \n",
    "\n",
    "    #replacing\n",
    "    df_copy[close_value_col_index][0:len(predictions)] = predicted_normalised_values_df[0].values\n",
    "    \n",
    "    temp = normalizer.inverse_transform(df_copy)\n",
    "    df_copy = pd.DataFrame(temp)\n",
    "    return df_copy[close_value_col_index][0:len(predictions)]\n",
    "\n",
    "def make_train_test_val_sets(dataarray, num_features, close_value_col_index, time_step, train_per, val_per, test_per):\n",
    "    n = dataarray.shape[0] \n",
    "\n",
    "    #slices of data into time_steps \n",
    "    X_slice=[]\n",
    "    y_slice=[]\n",
    "    #normalized df dim 3125, 13\n",
    "    # i = 30 - 3125\n",
    "\n",
    "    for i in range(time_step, n):\n",
    "        X_slice.append(dataarray[ i-time_step:i ,  close_value_col_index] )      #1 example having data from 30 days dim 30x13\n",
    "        y_slice.append(dataarray[i,close_value_col_index])                  #close value data of the 31st day \n",
    "\n",
    "    #splitting percentage\n",
    "\n",
    "    #splitting slices for test,val,train and converting into np array\n",
    "    X_train=np.array( X_slice[ 0:int(n*train_per) ])\n",
    "    y_train=np.array( y_slice[ 0:int(n*train_per) ])\n",
    "\n",
    "    X_val=np.array( X_slice[ int(n*train_per):int(n*(train_per+val_per)) ])\n",
    "    y_val=np.array( y_slice[ int(n*train_per):int(n*(train_per+val_per)) ])\n",
    "\n",
    "    X_test=np.array( X_slice[ int(n*(train_per+val_per)):int(n*(train_per+val_per+test_per))])\n",
    "    y_test=np.array( y_slice[ int(n*(train_per+val_per)):int(n*(train_per+val_per+test_per))])\n",
    "    \n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], num_features))    \n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], X_val.shape[1], num_features))    \n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], num_features))    \n",
    "    #print(np.array(X_train).shape)\n",
    "    #X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], num_features))    \n",
    "    #dimension of X_train 3095, 30, 13\n",
    "    #dimension of y_train  \n",
    "    #print(np.array(X_train).shape)\n",
    "    return [X_train, y_train, X_val, y_val, X_test, y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Dividends</th>\n",
       "      <th>Stock Splits</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2009-12-31</th>\n",
       "      <td>24.073285</td>\n",
       "      <td>24.081056</td>\n",
       "      <td>23.684755</td>\n",
       "      <td>23.684755</td>\n",
       "      <td>31929700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-04</th>\n",
       "      <td>23.793539</td>\n",
       "      <td>24.166527</td>\n",
       "      <td>23.770227</td>\n",
       "      <td>24.049969</td>\n",
       "      <td>38409100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-05</th>\n",
       "      <td>23.972268</td>\n",
       "      <td>24.166532</td>\n",
       "      <td>23.809084</td>\n",
       "      <td>24.057743</td>\n",
       "      <td>49749600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-06</th>\n",
       "      <td>23.995573</td>\n",
       "      <td>24.150985</td>\n",
       "      <td>23.715832</td>\n",
       "      <td>23.910097</td>\n",
       "      <td>58182400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-07</th>\n",
       "      <td>23.801302</td>\n",
       "      <td>23.855697</td>\n",
       "      <td>23.459397</td>\n",
       "      <td>23.661432</td>\n",
       "      <td>50559700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-17</th>\n",
       "      <td>165.284340</td>\n",
       "      <td>165.333705</td>\n",
       "      <td>163.319719</td>\n",
       "      <td>164.968430</td>\n",
       "      <td>34371700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-21</th>\n",
       "      <td>164.553765</td>\n",
       "      <td>166.044512</td>\n",
       "      <td>164.306954</td>\n",
       "      <td>164.376068</td>\n",
       "      <td>29517200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-22</th>\n",
       "      <td>165.264584</td>\n",
       "      <td>165.353448</td>\n",
       "      <td>163.566524</td>\n",
       "      <td>163.586273</td>\n",
       "      <td>24138800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-23</th>\n",
       "      <td>164.070024</td>\n",
       "      <td>164.672243</td>\n",
       "      <td>163.161761</td>\n",
       "      <td>164.593262</td>\n",
       "      <td>19680800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-24</th>\n",
       "      <td>165.373186</td>\n",
       "      <td>165.392935</td>\n",
       "      <td>162.352222</td>\n",
       "      <td>162.934692</td>\n",
       "      <td>24918100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2533 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Open        High         Low       Close    Volume  \\\n",
       "Date                                                                   \n",
       "2009-12-31   24.073285   24.081056   23.684755   23.684755  31929700   \n",
       "2010-01-04   23.793539   24.166527   23.770227   24.049969  38409100   \n",
       "2010-01-05   23.972268   24.166532   23.809084   24.057743  49749600   \n",
       "2010-01-06   23.995573   24.150985   23.715832   23.910097  58182400   \n",
       "2010-01-07   23.801302   23.855697   23.459397   23.661432  50559700   \n",
       "...                ...         ...         ...         ...       ...   \n",
       "2020-01-17  165.284340  165.333705  163.319719  164.968430  34371700   \n",
       "2020-01-21  164.553765  166.044512  164.306954  164.376068  29517200   \n",
       "2020-01-22  165.264584  165.353448  163.566524  163.586273  24138800   \n",
       "2020-01-23  164.070024  164.672243  163.161761  164.593262  19680800   \n",
       "2020-01-24  165.373186  165.392935  162.352222  162.934692  24918100   \n",
       "\n",
       "            Dividends  Stock Splits  \n",
       "Date                                 \n",
       "2009-12-31        0.0             0  \n",
       "2010-01-04        0.0             0  \n",
       "2010-01-05        0.0             0  \n",
       "2010-01-06        0.0             0  \n",
       "2010-01-07        0.0             0  \n",
       "...               ...           ...  \n",
       "2020-01-17        0.0             0  \n",
       "2020-01-21        0.0             0  \n",
       "2020-01-22        0.0             0  \n",
       "2020-01-23        0.0             0  \n",
       "2020-01-24        0.0             0  \n",
       "\n",
       "[2533 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tickerSymbol = 'MSFT'\n",
    "tickerData = yf.Ticker(tickerSymbol)\n",
    "stock_market_dataset = tickerData.history(period='1d', start='2010-1-1', end='2020-1-25')\n",
    "stock_market_dataset\n",
    "#data_set_file = 'Google.csv'\n",
    "#stock_market_dataset = pd.read_csv(data_set_file)\n",
    "#stock_market_dataset.shape\n",
    "#stock_market_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37, 30, 1) training dataset shape\n",
      "(38, 30, 1)  val dataset shape\n",
      "(9, 30, 1)  test dataset shape\n"
     ]
    }
   ],
   "source": [
    "###split data set into train, cross validation and test set\n",
    "\n",
    "num_features = 6 \n",
    "close_value_col_index = 4\n",
    "select_cols = stock_market_dataset[['Close']]\n",
    "new_df = select_cols.copy()\n",
    "\n",
    "###normalize data\n",
    "normalized_array, normalizer = normalize_dataset(new_df)\n",
    "#normalized_df.head()\n",
    "#n = normalized_df.shape[0]\n",
    "\n",
    "#creating training set with time steps.\n",
    "\n",
    "time_step=30    #1 month time step\n",
    "train_per = 0.015\n",
    "val_per = 0.015\n",
    "test_per = 0.0035\n",
    "    \n",
    "X_train, y_train, X_val, y_val, X_test, y_test =  make_train_test_val_sets(normalized_array, 1, 0, time_step, train_per, val_per, test_per)\n",
    "\n",
    "print(str(X_train.shape)+ \" training dataset shape\")\n",
    "print(str(X_val.shape)+ \"  val dataset shape\")\n",
    "print(str(X_test.shape)+ \"  test dataset shape\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LstmParam' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-9663e4215524>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mmem_cell_ct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mx_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mlstm_param\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLstmParam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmem_cell_ct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mlstm_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLstmNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_param\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0my_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LstmParam' is not defined"
     ]
    }
   ],
   "source": [
    "class LossLayer:\n",
    "    \"\"\"\n",
    "    Computes square loss with first element of hidden layer array.\n",
    "    \"\"\"\n",
    "    @classmethod\n",
    "    def loss(self, pred, label):\n",
    "        return (pred[0] - label) ** 2\n",
    "\n",
    "    @classmethod\n",
    "    def derivative(self, pred, label):\n",
    "        diff = np.zeros_like(pred)\n",
    "        diff[0] = 2 * (pred[0] - label)\n",
    "        return diff\n",
    "    \n",
    "    def percentage_error(pred, label):\n",
    "        return (100/len(pred))*np.sum(np.absolute(np.array(label)-np.array(pred))/np.array(label))\n",
    "\n",
    "\n",
    "# learns to repeat simple sequence from random inputs\n",
    "np.random.seed(0)\n",
    "\n",
    "# parameters for input data dimension and lstm cell count\n",
    "mem_cell_ct = 100\n",
    "x_dim = time_step\n",
    "lstm_param = LstmParam(mem_cell_ct, x_dim)\n",
    "lstm_net = LstmNetwork(lstm_param)\n",
    "y_list = y_train.tolist()\n",
    "input_val_arr = X_train.tolist()\n",
    "for cur_iter in range(1200):\n",
    "    if ((cur_iter+1)%100 == 0):\n",
    "        print(\"iter\", \"%2s\" % str(cur_iter+1), end=\": \")\n",
    "    for ind in range(len(y_list)):\n",
    "        lstm_net.add_x_list(input_val_arr[ind])\n",
    "\n",
    "    #print(\"y_pred = [\" +\n",
    "    #      \", \".join(\"[% 2.5f % 2.5f]\" % (lstm_net.lstm_layer_list[ind].state.h[0], y_list[ind]) for ind in range(len(y_list))) +\n",
    "    #      \"]\", end=\", \")\n",
    "\n",
    "    loss = lstm_net.get_loss(y_list, LossLayer)\n",
    "    if ((cur_iter+1)%100 == 0):\n",
    "        print(\"loss:\", \"%.3e\" % loss)\n",
    "    lstm_param.apply_derivatives(0.1)\n",
    "    lstm_net.clear_x_list()\n",
    "\n",
    "pred = []\n",
    "for ind in range(len(y_list)):\n",
    "    pred.append(lstm_net.lstm_layer_list[ind].state.h[0])\n",
    "\n",
    "print(\"Accuracy: % 2.2f%%\" % (100-LossLayer.percentage_error(pred, y_list)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross Val \n",
    "y_list = y_val.tolist()\n",
    "input_val_arr = X_val.tolist()\n",
    "for ind in range(len(y_list)):\n",
    "    lstm_net.add_x_list(input_val_arr[ind])\n",
    "\n",
    "pred = []\n",
    "for ind in range(len(y_list)):\n",
    "    pred.append(lstm_net.lstm_layer_list[ind].state.h[0])\n",
    "\n",
    "print(len(pred))\n",
    "lstm_net.clear_x_list()\n",
    "\n",
    "print(\"Accuracy: % 2.2f%%\" % (100-LossLayer.percentage_error(pred, y_list)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test  \n",
    "y_list = y_test.tolist()\n",
    "input_val_arr = X_test.tolist()\n",
    "for ind in range(len(y_list)):\n",
    "    lstm_net.add_x_list(input_val_arr[ind])\n",
    "\n",
    "pred = []\n",
    "for ind in range(len(y_list)):\n",
    "    pred.append(lstm_net.lstm_layer_list[ind].state.h[0])\n",
    "\n",
    "print(len(pred))\n",
    "lstm_net.clear_x_list()\n",
    "print(\"Accuracy: % 2.2f%%\" % (100-LossLayer.percentage_error(pred, y_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denormed = denormalize_data(pd.DataFrame(normalized_array), normalizer, time_step, pred, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(df, day_index, time_step, lstm_net, y_list):   #date should be greater than time_step\n",
    "    actual = df['Close'][day_index]    \n",
    "    select_cols = df[['Close']]\n",
    "    new_df = select_cols.copy()\n",
    "    normalized, normalizer = normalize_dataset(new_df)\n",
    "    norm_actual = normalized[day_index][0]\n",
    "    y_list.append(norm_actual)\n",
    "    sequence = [normalized[i] for i in range(day_index-time_step,day_index)]\n",
    "    lstm_net.add_x_list(np.reshape(np.array(sequence), (time_step)).tolist())\n",
    "    pred = lstm_net.lstm_layer_list[len(lstm_net.lstm_layer_list)-1].state.h[0]\n",
    "    normalized[day_index][0] = pred\n",
    "    temp = normalizer.inverse_transform(pd.DataFrame(normalized))\n",
    "    lstm_net.clear_x_list()\n",
    "    #print(temp[day_index][0], actual)\n",
    "    return temp[day_index][0], actual\n",
    "    \n",
    "\n",
    "get_prediction(stock_market_dataset, 50, 30, lstm_net, y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1773, 30, 1) training dataset shape\n",
      "(506, 30, 1)  val dataset shape\n",
      "(224, 30, 1)  test dataset shape\n",
      "Epoch 1/10\n",
      "178/178 [==============================] - 29s 119ms/step - loss: 0.0101\n",
      "Epoch 2/10\n",
      "178/178 [==============================] - 21s 118ms/step - loss: 0.0028\n",
      "Epoch 3/10\n",
      "178/178 [==============================] - 22s 122ms/step - loss: 0.0014\n",
      "Epoch 4/10\n",
      "178/178 [==============================] - 22s 123ms/step - loss: 0.0010\n",
      "Epoch 5/10\n",
      "178/178 [==============================] - 21s 118ms/step - loss: 8.1261e-04\n",
      "Epoch 6/10\n",
      "178/178 [==============================] - 20s 115ms/step - loss: 6.0846e-04\n",
      "Epoch 7/10\n",
      "178/178 [==============================] - 21s 116ms/step - loss: 4.9315e-04\n",
      "Epoch 8/10\n",
      "178/178 [==============================] - 20s 110ms/step - loss: 4.1236e-04\n",
      "Epoch 9/10\n",
      "178/178 [==============================] - 22s 123ms/step - loss: 3.5623e-04\n",
      "Epoch 10/10\n",
      "178/178 [==============================] - 22s 121ms/step - loss: 3.2922e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f84f44ae430>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prediction With keras\n",
    "train_per = 0.7\n",
    "val_per = 0.2\n",
    "test_per = 0.1\n",
    "X_train, y_train, X_val, y_val, X_test, y_test =  make_train_test_val_sets(normalized_array, 1, 0, time_step, train_per, val_per, test_per)\n",
    "\n",
    "print(str(X_train.shape)+ \" training dataset shape\")\n",
    "print(str(X_val.shape)+ \"  val dataset shape\")\n",
    "print(str(X_test.shape)+ \"  test dataset shape\")\n",
    "#Implementing LSTM\n",
    "model = keras.models.Sequential()         #initializing network\n",
    "\n",
    "#input layer\n",
    "hidden_layer_units = 10\n",
    "model.add(keras.layers.LSTM(hidden_layer_units,kernel_regularizer=L1L2(0.0001), return_sequences=True, input_shape=(X_train.shape[1],1)))\n",
    "\n",
    "#LSTM layer 2\n",
    "hidden_layer_units = 10\n",
    "model.add(keras.layers.LSTM(hidden_layer_units,kernel_regularizer=L1L2(0.0001)))\n",
    "\n",
    "#Output layer\n",
    "output_layer_units = 1     #just need the close value\n",
    "model.add(keras.layers.Dense(output_layer_units))\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "model.fit(X_train, y_train, epochs = 70, batch_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2503, 0)\n",
      "(2503,)\n"
     ]
    }
   ],
   "source": [
    "real_close_value = pd.DataFrame(normalized_array).iloc[ time_step: , close_value_col_index:close_value_col_index+1].values     #Close column normalized real value. Ideally predictions should be on test set.\n",
    "\n",
    "print(real_close_value.shape)\n",
    "\n",
    "predicted_close_value_train = model.predict(X_train)\n",
    "predicted_close_value_val = model.predict(X_val)\n",
    "predicted_close_value_test  = model.predict(X_test)   \n",
    "combined_prediction = np.append(predicted_close_value_train,np.append(predicted_close_value_val,predicted_close_value_test))\n",
    "\n",
    "print(combined_prediction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               0\n",
      "0      23.684755\n",
      "1      24.049969\n",
      "2      24.057743\n",
      "3      23.910097\n",
      "4      23.661432\n",
      "...          ...\n",
      "2528  164.968430\n",
      "2529  164.376068\n",
      "2530  163.586273\n",
      "2531  164.593262\n",
      "2532  162.934692\n",
      "\n",
      "[2533 rows x 1 columns]\n",
      "(2503, 1) (2503, 1)\n"
     ]
    }
   ],
   "source": [
    "temp = normalizer.inverse_transform(pd.DataFrame(normalized_array))\n",
    "real_close_value = pd.DataFrame(temp)\n",
    "temp = normalizer.inverse_transform(pd.DataFrame(combined_prediction))\n",
    "predicted_close_value = pd.DataFrame(temp)\n",
    "\n",
    "rows_to_drop = [i for i in range(0,time_step)]\n",
    "real_close_value.drop(rows_to_drop,inplace = True)     #deleting first 30 rows of normalized_df. Because output prediction for the first 30 days (first time slice) doesn't exist.\n",
    "print(predicted_close_value.shape, real_close_value.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_next_day_pred(day_index, label, pred):\n",
    "    x_coords = [day_index, day_index+1]\n",
    "    y_coords_label = [label[day_index][0], label[day_index+1][0]]\n",
    "    y_coords_pred = [pred[day_index][0], pred[day_index+1][0]]\n",
    "    plt.plot(x_coords, y_coords_label, color = 'blue', label = 'Close')\n",
    "    plt.plot(x_coords, y_coords_pred, color = 'green', label = 'Predicted Close')    \n",
    "    plt.xticks(np.arange(day_index, day_index+2, 1))\n",
    "    plt.title('Previous Day vs Today')\n",
    "    plt.ylabel('Day')\n",
    "    plt.ylabel('Close')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "get_next_day_pred(31, np.array(real_close_value), np.array(predicted_close_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lstmenv",
   "language": "python",
   "name": "lstmenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
