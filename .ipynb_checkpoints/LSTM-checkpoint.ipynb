{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.regularizers import L1L2\n",
    "#import project modules\n",
    "from modules import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_arr(a, b, *args): \n",
    "    np.random.seed(0)\n",
    "    return np.random.rand(*args) * (b - a) + a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmParam:\n",
    "    def __init__(self, ct_dim, x_dim):\n",
    "        \"\"\"\n",
    "        \n",
    "        initialize all weights and biases \n",
    "        ct_dim - the dimension of current cell state (Ct) matrix\n",
    "        x_dim - the dimension of Tth input\n",
    "        \n",
    "        Weight and bias matrices are initialized with random values instead of zeroes to add noise. \n",
    "        Their derivatives will thus be zero.\n",
    "        \n",
    "        Terminology:\n",
    "        \n",
    "        Prefixes\n",
    "        w - a weight matrix\n",
    "        b - a bias matrix\n",
    "        d - a derivative matrix\n",
    "        \n",
    "        Suffixes\n",
    "        c - cell state gate. Represents data held in current cell. \n",
    "            It is the c't (c bar t) matrix\n",
    "        i - input gate\n",
    "        f - forget gate\n",
    "        o - output gate        \n",
    "        \n",
    "        \"\"\"\n",
    "        self.ct_dim = ct_dim  \n",
    "        self.x_dim = x_dim \n",
    "        combined_dim = x_dim + ct_dim\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.wc = rand_arr(-0.1, 0.1, ct_dim, combined_dim)\n",
    "        self.wi = rand_arr(-0.1, 0.1, ct_dim, combined_dim) \n",
    "        self.wf = rand_arr(-0.1, 0.1, ct_dim, combined_dim)\n",
    "        self.wo = rand_arr(-0.1, 0.1, ct_dim, combined_dim)\n",
    "        \n",
    "        #Initialize biases \n",
    "        self.bc = rand_arr(-0.1, 0.1, ct_dim) \n",
    "        self.bi = rand_arr(-0.1, 0.1, ct_dim) \n",
    "        self.bf = rand_arr(-0.1, 0.1, ct_dim) \n",
    "        self.bo = rand_arr(-0.1, 0.1, ct_dim) \n",
    "        \n",
    "        # Initialize derivatives\n",
    "        self.dwc = np.zeros((ct_dim, combined_dim)) \n",
    "        self.dwi = np.zeros((ct_dim, combined_dim)) \n",
    "        self.dwf = np.zeros((ct_dim, combined_dim)) \n",
    "        self.dwo = np.zeros((ct_dim, combined_dim)) \n",
    "        self.dbc = np.zeros(ct_dim) \n",
    "        self.dbi = np.zeros(ct_dim) \n",
    "        self.dbf = np.zeros(ct_dim) \n",
    "        self.dbo = np.zeros(ct_dim)\n",
    "        \n",
    "    def apply_derivatives(self, alpha = 1):\n",
    "        \"\"\"\n",
    "        Update parameters in each iteration to reach optimal value \n",
    "        alpha is the learning rate.\n",
    "        \"\"\"\n",
    "        self.wc -= alpha * self.dwc\n",
    "        self.wi -= alpha * self.dwi\n",
    "        self.wf -= alpha * self.dwf\n",
    "        self.wo -= alpha * self.dwo\n",
    "        self.bc -= alpha * self.dbc\n",
    "        self.bi -= alpha * self.dbi\n",
    "        self.bf -= alpha * self.dbf\n",
    "        self.bo -= alpha * self.dbo\n",
    "        \n",
    "        # reset all derivatives to zero\n",
    "        self.dwc = np.zeros_like(self.wc)\n",
    "        self.dwi = np.zeros_like(self.wi) \n",
    "        self.dwf = np.zeros_like(self.wf) \n",
    "        self.dwo = np.zeros_like(self.wo) \n",
    "        self.dbc = np.zeros_like(self.bc)\n",
    "        self.dbi = np.zeros_like(self.bi) \n",
    "        self.dbf = np.zeros_like(self.bf) \n",
    "        self.dbo = np.zeros_like(self.bo) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmCellState:\n",
    "    def __init__(self, ct_dim, x_dim):\n",
    "        \"\"\"\n",
    "        Initialize all gate matrices. \n",
    "        All gate matrices have the same dimension as ct matrix\n",
    "        c - current hidden cell state. The gate corresponding to this determines how much \n",
    "            data of previous cell should be read in current cell.\n",
    "        i - input gate. Determines how much data should be read into the cell from current input.\n",
    "        f - forget gate. Determines how much data should be forgotten, i.e discarded\n",
    "        o - output gate. How much data to output from current cell to next cell\n",
    "        s - The present state of gate. \n",
    "            Equation to calculate present state : forget_gate*previous_state(s<t-1>) + c_gate*input_gate\n",
    "        h - output state of the cell. It is the prediction value of Tth output in series.\n",
    "        dh - derivative of h\n",
    "        ds - derivative of s\n",
    "        \"\"\"\n",
    "        self.c = np.zeros(ct_dim)\n",
    "        self.i = np.zeros(ct_dim)\n",
    "        self.f = np.zeros(ct_dim)\n",
    "        self.o = np.zeros(ct_dim)\n",
    "        self.s = np.zeros(ct_dim)\n",
    "        self.h = np.zeros(ct_dim)\n",
    "        self.dh = np.zeros_like(self.h)\n",
    "        self.ds = np.zeros_like(self.s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmLayer:\n",
    "    def __init__(self, lstm_param, lstm_cell_state):\n",
    "        self.state = lstm_cell_state\n",
    "        self.param = lstm_param\n",
    "        #xh is the concatenation ofprevious layer's output with current input.\n",
    "        self.xh = None\n",
    "\n",
    "    def calculate_gates(self, x, s_prev = None, h_prev = None):\n",
    "        \"\"\"\n",
    "        if this is the first lstm layer in the network then \n",
    "        s_prev and h_prev will be initialized to zero as cell state \n",
    "        and output states are not present.\n",
    "        \n",
    "        s_prev = cell state of previous layer's cells.\n",
    "        h_prev = output state from previous layer\n",
    "        \n",
    "        \"\"\"\n",
    "        if s_prev is None: \n",
    "            s_prev = np.zeros_like(self.state.s)\n",
    "        if h_prev is None: \n",
    "            h_prev = np.zeros_like(self.state.h)\n",
    "        \n",
    "        # save previous states for use in backprop\n",
    "        self.s_prev = s_prev\n",
    "        self.h_prev = h_prev\n",
    "\n",
    "        # concatenate x(T) and h(T-1)\n",
    "        xh = np.hstack((x,  h_prev))\n",
    "        #Calculate gate values\n",
    "        self.state.c = tanh(np.dot(self.param.wc, xh) + self.param.bc)\n",
    "        self.state.i = sigmoid(np.dot(self.param.wi, xh) + self.param.bi)\n",
    "        self.state.f = sigmoid(np.dot(self.param.wf, xh) + self.param.bf)\n",
    "        self.state.o = sigmoid(np.dot(self.param.wo, xh) + self.param.bo)\n",
    "        self.state.s = self.state.c * self.state.i + s_prev * self.state.f\n",
    "        self.state.h = self.state.s * self.state.o\n",
    "        self.xh = xh\n",
    "    \n",
    "    def calculate_derivatives(self, dh, ds):\n",
    "        ds = self.state.o * dh + ds\n",
    "        do = self.state.s * dh\n",
    "        di = self.state.c * ds\n",
    "        dc = self.state.i * ds\n",
    "        df = self.s_prev * ds\n",
    "\n",
    "        # calculate derivatives w.r.t. gate inside sigma / tanh function\n",
    "        di_input = dsigmoid(self.state.i) * di \n",
    "        df_input = dsigmoid(self.state.f) * df \n",
    "        do_input = dsigmoid(self.state.o) * do \n",
    "        dc_input = dtanh(self.state.c) * dc\n",
    "\n",
    "        # derivatives w.r.t. inputs\n",
    "        self.param.dwc += np.outer(dc_input, self.xh)\n",
    "        self.param.dwi += np.outer(di_input, self.xh)\n",
    "        self.param.dwf += np.outer(df_input, self.xh)\n",
    "        self.param.dwo += np.outer(do_input, self.xh)\n",
    "        self.param.dbc += dc_input       \n",
    "        self.param.dbi += di_input\n",
    "        self.param.dbf += df_input       \n",
    "        self.param.dbo += do_input\n",
    "        \n",
    "        # calculate derivative for xh\n",
    "        dxh = np.zeros_like(self.xh)\n",
    "        dxh += np.dot(self.param.wc.T, dc_input)\n",
    "        dxh += np.dot(self.param.wi.T, di_input)\n",
    "        dxh += np.dot(self.param.wf.T, df_input)\n",
    "        dxh += np.dot(self.param.wo.T, do_input)\n",
    "        \n",
    "        # save derivatives\n",
    "        self.state.ds = ds * self.state.f\n",
    "        self.state.dh = dxh[self.param.x_dim:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmNetwork():\n",
    "    def __init__(self, lstm_param):\n",
    "        \"\"\"\n",
    "        x_list - the sequence that'll be iput to lstm\n",
    "        lstm_layer_list - the ouput from layer that will be input to next layer\n",
    "        \"\"\"\n",
    "        self.lstm_param = lstm_param\n",
    "        self.lstm_layer_list = []\n",
    "        self.x_list = []\n",
    "\n",
    "    def get_loss(self, y_list, loss_layer):\n",
    "        \"\"\"\n",
    "        Updates derivatives w.r.t corresponding loss layer. \n",
    "        To update parameters, we will call self.lstm_param.apply_derivatives()\n",
    "        \"\"\"\n",
    "        index = len(self.x_list) - 1\n",
    "        # Calculate loss for the last layer \n",
    "        loss = loss_layer.loss(self.lstm_layer_list[index].state.h, y_list[index])\n",
    "        diff_h = loss_layer.derivative(self.lstm_layer_list[index].state.h, y_list[index])\n",
    "        # For the last layer of the network, diff_s will be\n",
    "        diff_s = np.zeros(self.lstm_param.ct_dim)\n",
    "        self.lstm_layer_list[index].calculate_derivatives(diff_h, diff_s)\n",
    "        index -= 1\n",
    "\n",
    "        while index >= 0:\n",
    "            loss += loss_layer.loss(self.lstm_layer_list[index].state.h, y_list[index])\n",
    "            diff_h = loss_layer.derivative(self.lstm_layer_list[index].state.h, y_list[index])\n",
    "            diff_h += self.lstm_layer_list[index + 1].state.dh\n",
    "            diff_s = self.lstm_layer_list[index + 1].state.ds\n",
    "            self.lstm_layer_list[index].calculate_derivatives(diff_h, diff_s)\n",
    "            index -= 1 \n",
    "\n",
    "        return loss\n",
    "\n",
    "    def clear_x_list(self):\n",
    "        self.x_list = []\n",
    "\n",
    "    def add_x_list(self, x):\n",
    "        self.x_list.append(x)\n",
    "        if len(self.x_list) > len(self.lstm_layer_list):\n",
    "            lstm_state = LstmCellState(self.lstm_param.ct_dim, self.lstm_param.x_dim)\n",
    "            self.lstm_layer_list.append(LstmLayer(self.lstm_param, lstm_state))\n",
    "\n",
    "        # get index of most recent x input\n",
    "        index = len(self.x_list) - 1\n",
    "        if index == 0:\n",
    "            self.lstm_layer_list[index].calculate_gates(x)\n",
    "        else:\n",
    "            s_prev = self.lstm_layer_list[index - 1].state.s\n",
    "            h_prev = self.lstm_layer_list[index - 1].state.h\n",
    "            self.lstm_layer_list[index].calculate_gates(x, s_prev, h_prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dataset(stock_market_dataset):\n",
    "    normalizer = preprocessing.MinMaxScaler((0,1))\n",
    "    smd_exclude_strings = stock_market_dataset.select_dtypes(include = [np.number])\n",
    "    normalized = normalizer.fit_transform(smd_exclude_strings)\n",
    "    return [normalized, normalizer]\n",
    "\n",
    "def denormalize_data(df, normalizer, time_step, predictions, close_value_col_index):\n",
    "    df_copy = df.copy(deep = True)\n",
    "    rows_to_drop = [i for i in range(0,time_step)]\n",
    "    df_copy.drop(rows_to_drop,inplace = True)     \n",
    "    #deleting first #time_step rows of normalized_df. Because output prediction for the first 30 days (first time slice) doesn't exist.\n",
    "\n",
    "    #converting prediction array to dataframe for replacing actual row with prediction row\n",
    "    predicted_normalised_values_df = pd.DataFrame(predictions) \n",
    "\n",
    "    #replacing\n",
    "    df_copy[close_value_col_index][0:len(predictions)] = predicted_normalised_values_df[0].values\n",
    "    \n",
    "    temp = normalizer.inverse_transform(df_copy)\n",
    "    df_copy = pd.DataFrame(temp)\n",
    "    return df_copy[close_value_col_index][0:len(predictions)]\n",
    "\n",
    "def make_train_test_val_sets(dataarray, num_features, close_value_col_index, time_step, train_per, val_per, test_per):\n",
    "    n = dataarray.shape[0] \n",
    "\n",
    "    #slices of data into time_steps \n",
    "    X_slice=[]\n",
    "    y_slice=[]\n",
    "    #normalized df dim 3125, 13\n",
    "    # i = 30 - 3125\n",
    "\n",
    "    for i in range(time_step, n):\n",
    "        X_slice.append(dataarray[ i-time_step:i ,  close_value_col_index] )      #1 example having data from 30 days dim 30x13\n",
    "        y_slice.append(dataarray[i,close_value_col_index])                  #close value data of the 31st day \n",
    "\n",
    "    #splitting percentage\n",
    "\n",
    "    #splitting slices for test,val,train and converting into np array\n",
    "    X_train=np.array( X_slice[ 0:int(n*train_per) ])\n",
    "    y_train=np.array( y_slice[ 0:int(n*train_per) ])\n",
    "\n",
    "    X_val=np.array( X_slice[ int(n*train_per):int(n*(train_per+val_per)) ])\n",
    "    y_val=np.array( y_slice[ int(n*train_per):int(n*(train_per+val_per)) ])\n",
    "\n",
    "    X_test=np.array( X_slice[ int(n*(train_per+val_per)):int(n*(train_per+val_per+test_per))])\n",
    "    y_test=np.array( y_slice[ int(n*(train_per+val_per)):int(n*(train_per+val_per+test_per))])\n",
    "    \n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], num_features))    \n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], X_val.shape[1], num_features))    \n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], num_features))    \n",
    "    #print(np.array(X_train).shape)\n",
    "    #X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], num_features))    \n",
    "    #dimension of X_train 3095, 30, 13\n",
    "    #dimension of y_train  \n",
    "    #print(np.array(X_train).shape)\n",
    "    return [X_train, y_train, X_val, y_val, X_test, y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Dividends</th>\n",
       "      <th>Stock Splits</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2009-12-31</th>\n",
       "      <td>24.073285</td>\n",
       "      <td>24.081056</td>\n",
       "      <td>23.684755</td>\n",
       "      <td>23.684755</td>\n",
       "      <td>31929700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-04</th>\n",
       "      <td>23.793539</td>\n",
       "      <td>24.166527</td>\n",
       "      <td>23.770227</td>\n",
       "      <td>24.049969</td>\n",
       "      <td>38409100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-05</th>\n",
       "      <td>23.972268</td>\n",
       "      <td>24.166532</td>\n",
       "      <td>23.809084</td>\n",
       "      <td>24.057743</td>\n",
       "      <td>49749600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-06</th>\n",
       "      <td>23.995573</td>\n",
       "      <td>24.150985</td>\n",
       "      <td>23.715832</td>\n",
       "      <td>23.910097</td>\n",
       "      <td>58182400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-07</th>\n",
       "      <td>23.801302</td>\n",
       "      <td>23.855697</td>\n",
       "      <td>23.459397</td>\n",
       "      <td>23.661432</td>\n",
       "      <td>50559700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-17</th>\n",
       "      <td>165.284340</td>\n",
       "      <td>165.333705</td>\n",
       "      <td>163.319719</td>\n",
       "      <td>164.968430</td>\n",
       "      <td>34371700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-21</th>\n",
       "      <td>164.553765</td>\n",
       "      <td>166.044512</td>\n",
       "      <td>164.306954</td>\n",
       "      <td>164.376068</td>\n",
       "      <td>29517200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-22</th>\n",
       "      <td>165.264584</td>\n",
       "      <td>165.353448</td>\n",
       "      <td>163.566524</td>\n",
       "      <td>163.586273</td>\n",
       "      <td>24138800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-23</th>\n",
       "      <td>164.070024</td>\n",
       "      <td>164.672243</td>\n",
       "      <td>163.161761</td>\n",
       "      <td>164.593262</td>\n",
       "      <td>19680800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-24</th>\n",
       "      <td>165.373186</td>\n",
       "      <td>165.392935</td>\n",
       "      <td>162.352222</td>\n",
       "      <td>162.934692</td>\n",
       "      <td>24918100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2533 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Open        High         Low       Close    Volume  \\\n",
       "Date                                                                   \n",
       "2009-12-31   24.073285   24.081056   23.684755   23.684755  31929700   \n",
       "2010-01-04   23.793539   24.166527   23.770227   24.049969  38409100   \n",
       "2010-01-05   23.972268   24.166532   23.809084   24.057743  49749600   \n",
       "2010-01-06   23.995573   24.150985   23.715832   23.910097  58182400   \n",
       "2010-01-07   23.801302   23.855697   23.459397   23.661432  50559700   \n",
       "...                ...         ...         ...         ...       ...   \n",
       "2020-01-17  165.284340  165.333705  163.319719  164.968430  34371700   \n",
       "2020-01-21  164.553765  166.044512  164.306954  164.376068  29517200   \n",
       "2020-01-22  165.264584  165.353448  163.566524  163.586273  24138800   \n",
       "2020-01-23  164.070024  164.672243  163.161761  164.593262  19680800   \n",
       "2020-01-24  165.373186  165.392935  162.352222  162.934692  24918100   \n",
       "\n",
       "            Dividends  Stock Splits  \n",
       "Date                                 \n",
       "2009-12-31        0.0             0  \n",
       "2010-01-04        0.0             0  \n",
       "2010-01-05        0.0             0  \n",
       "2010-01-06        0.0             0  \n",
       "2010-01-07        0.0             0  \n",
       "...               ...           ...  \n",
       "2020-01-17        0.0             0  \n",
       "2020-01-21        0.0             0  \n",
       "2020-01-22        0.0             0  \n",
       "2020-01-23        0.0             0  \n",
       "2020-01-24        0.0             0  \n",
       "\n",
       "[2533 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tickerSymbol = 'MSFT'\n",
    "tickerData = yf.Ticker(tickerSymbol)\n",
    "stock_market_dataset = tickerData.history(period='1d', start='2010-1-1', end='2020-1-25')\n",
    "stock_market_dataset\n",
    "#data_set_file = 'Google.csv'\n",
    "#stock_market_dataset = pd.read_csv(data_set_file)\n",
    "#stock_market_dataset.shape\n",
    "#stock_market_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37, 30, 1) training dataset shape\n",
      "(38, 30, 1)  val dataset shape\n",
      "(9, 30, 1)  test dataset shape\n"
     ]
    }
   ],
   "source": [
    "###split data set into train, cross validation and test set\n",
    "\n",
    "num_features = 6 \n",
    "close_value_col_index = 4\n",
    "select_cols = stock_market_dataset[['Close']]\n",
    "new_df = select_cols.copy()\n",
    "\n",
    "###normalize data\n",
    "normalized_array, normalizer = normalize_dataset(new_df)\n",
    "#normalized_df.head()\n",
    "#n = normalized_df.shape[0]\n",
    "\n",
    "#creating training set with time steps.\n",
    "\n",
    "time_step=30    #1 month time step\n",
    "train_per = 0.015\n",
    "val_per = 0.015\n",
    "test_per = 0.0035\n",
    "    \n",
    "X_train, y_train, X_val, y_val, X_test, y_test =  make_train_test_val_sets(normalized_array, 1, 0, time_step, train_per, val_per, test_per)\n",
    "\n",
    "print(str(X_train.shape)+ \" training dataset shape\")\n",
    "print(str(X_val.shape)+ \"  val dataset shape\")\n",
    "print(str(X_test.shape)+ \"  test dataset shape\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LstmParam' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-9663e4215524>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mmem_cell_ct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mx_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mlstm_param\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLstmParam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmem_cell_ct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mlstm_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLstmNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_param\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0my_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LstmParam' is not defined"
     ]
    }
   ],
   "source": [
    "class LossLayer:\n",
    "    \"\"\"\n",
    "    Computes square loss with first element of hidden layer array.\n",
    "    \"\"\"\n",
    "    @classmethod\n",
    "    def loss(self, pred, label):\n",
    "        return (pred[0] - label) ** 2\n",
    "\n",
    "    @classmethod\n",
    "    def derivative(self, pred, label):\n",
    "        diff = np.zeros_like(pred)\n",
    "        diff[0] = 2 * (pred[0] - label)\n",
    "        return diff\n",
    "    \n",
    "    def percentage_error(pred, label):\n",
    "        return (100/len(pred))*np.sum(np.absolute(np.array(label)-np.array(pred))/np.array(label))\n",
    "\n",
    "\n",
    "# learns to repeat simple sequence from random inputs\n",
    "np.random.seed(0)\n",
    "\n",
    "# parameters for input data dimension and lstm cell count\n",
    "mem_cell_ct = 100\n",
    "x_dim = time_step\n",
    "lstm_param = LstmParam(mem_cell_ct, x_dim)\n",
    "lstm_net = LstmNetwork(lstm_param)\n",
    "y_list = y_train.tolist()\n",
    "input_val_arr = X_train.tolist()\n",
    "for cur_iter in range(1200):\n",
    "    if ((cur_iter+1)%100 == 0):\n",
    "        print(\"iter\", \"%2s\" % str(cur_iter+1), end=\": \")\n",
    "    for ind in range(len(y_list)):\n",
    "        lstm_net.add_x_list(input_val_arr[ind])\n",
    "\n",
    "    #print(\"y_pred = [\" +\n",
    "    #      \", \".join(\"[% 2.5f % 2.5f]\" % (lstm_net.lstm_layer_list[ind].state.h[0], y_list[ind]) for ind in range(len(y_list))) +\n",
    "    #      \"]\", end=\", \")\n",
    "\n",
    "    loss = lstm_net.get_loss(y_list, LossLayer)\n",
    "    if ((cur_iter+1)%100 == 0):\n",
    "        print(\"loss:\", \"%.3e\" % loss)\n",
    "    lstm_param.apply_derivatives(0.1)\n",
    "    lstm_net.clear_x_list()\n",
    "\n",
    "pred = []\n",
    "for ind in range(len(y_list)):\n",
    "    pred.append(lstm_net.lstm_layer_list[ind].state.h[0])\n",
    "\n",
    "print(\"Accuracy: % 2.2f%%\" % (100-LossLayer.percentage_error(pred, y_list)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross Val \n",
    "y_list = y_val.tolist()\n",
    "input_val_arr = X_val.tolist()\n",
    "for ind in range(len(y_list)):\n",
    "    lstm_net.add_x_list(input_val_arr[ind])\n",
    "\n",
    "pred = []\n",
    "for ind in range(len(y_list)):\n",
    "    pred.append(lstm_net.lstm_layer_list[ind].state.h[0])\n",
    "\n",
    "print(len(pred))\n",
    "lstm_net.clear_x_list()\n",
    "\n",
    "print(\"Accuracy: % 2.2f%%\" % (100-LossLayer.percentage_error(pred, y_list)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test  \n",
    "y_list = y_test.tolist()\n",
    "input_val_arr = X_test.tolist()\n",
    "for ind in range(len(y_list)):\n",
    "    lstm_net.add_x_list(input_val_arr[ind])\n",
    "\n",
    "pred = []\n",
    "for ind in range(len(y_list)):\n",
    "    pred.append(lstm_net.lstm_layer_list[ind].state.h[0])\n",
    "\n",
    "print(len(pred))\n",
    "lstm_net.clear_x_list()\n",
    "print(\"Accuracy: % 2.2f%%\" % (100-LossLayer.percentage_error(pred, y_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denormed = denormalize_data(pd.DataFrame(normalized_array), normalizer, time_step, pred, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(df, day_index, time_step, lstm_net, y_list):   #date should be greater than time_step\n",
    "    actual = df['Close'][day_index]    \n",
    "    select_cols = df[['Close']]\n",
    "    new_df = select_cols.copy()\n",
    "    normalized, normalizer = normalize_dataset(new_df)\n",
    "    norm_actual = normalized[day_index][0]\n",
    "    y_list.append(norm_actual)\n",
    "    sequence = [normalized[i] for i in range(day_index-time_step,day_index)]\n",
    "    lstm_net.add_x_list(np.reshape(np.array(sequence), (time_step)).tolist())\n",
    "    pred = lstm_net.lstm_layer_list[len(lstm_net.lstm_layer_list)-1].state.h[0]\n",
    "    normalized[day_index][0] = pred\n",
    "    temp = normalizer.inverse_transform(pd.DataFrame(normalized))\n",
    "    lstm_net.clear_x_list()\n",
    "    #print(temp[day_index][0], actual)\n",
    "    return temp[day_index][0], actual\n",
    "    \n",
    "\n",
    "get_prediction(stock_market_dataset, 50, 30, lstm_net, y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1773, 30, 1) training dataset shape\n",
      "(506, 30, 1)  val dataset shape\n",
      "(224, 30, 1)  test dataset shape\n",
      "178/178 [==============================] - 30s 121ms/step - loss: 0.0110\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fdaf4168370>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prediction With keras\n",
    "train_per = 0.7\n",
    "val_per = 0.2\n",
    "test_per = 0.1\n",
    "X_train, y_train, X_val, y_val, X_test, y_test =  make_train_test_val_sets(normalized_array, 1, 0, time_step, train_per, val_per, test_per)\n",
    "\n",
    "print(str(X_train.shape)+ \" training dataset shape\")\n",
    "print(str(X_val.shape)+ \"  val dataset shape\")\n",
    "print(str(X_test.shape)+ \"  test dataset shape\")\n",
    "#Implementing LSTM\n",
    "model = keras.models.Sequential()         #initializing network\n",
    "\n",
    "#input layer\n",
    "hidden_layer_units = 10\n",
    "model.add(keras.layers.LSTM(hidden_layer_units,kernel_regularizer=L1L2(0.0001), return_sequences=True, input_shape=(X_train.shape[1],1)))\n",
    "\n",
    "#LSTM layer 2\n",
    "hidden_layer_units = 10\n",
    "model.add(keras.layers.LSTM(hidden_layer_units,kernel_regularizer=L1L2(0.0001)))\n",
    "\n",
    "#Output layer\n",
    "output_layer_units = 1     #just need the close value\n",
    "model.add(keras.layers.Dense(output_layer_units))\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "model.fit(X_train, y_train, epochs = 70, batch_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2503, 0)\n",
      "(224, 1) (224, 30, 1)\n"
     ]
    }
   ],
   "source": [
    "real_close_value = pd.DataFrame(normalized_array).iloc[ time_step: , close_value_col_index:close_value_col_index+1].values     #Close column normalized real value. Ideally predictions should be on test set.\n",
    "\n",
    "print(real_close_value.shape)\n",
    "\n",
    "predicted_close_value_train = model.predict(X_train)\n",
    "predicted_close_value_val = model.predict(X_val)\n",
    "predicted_close_value_test  = model.predict(X_test)   \n",
    "combined_prediction = np.append(predicted_close_value_train,np.append(predicted_close_value_val,predicted_close_value_test))\n",
    "\n",
    "print(predicted_close_value_test.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2503, 1) (2503, 1)\n"
     ]
    }
   ],
   "source": [
    "temp = normalizer.inverse_transform(pd.DataFrame(normalized_array))\n",
    "real_close_value = pd.DataFrame(temp)\n",
    "temp = normalizer.inverse_transform(pd.DataFrame(combined_prediction))\n",
    "predicted_close_value = pd.DataFrame(temp)\n",
    "\n",
    "rows_to_drop = [i for i in range(0,time_step)]\n",
    "real_close_value.drop(rows_to_drop,inplace = True)     #deleting first 30 rows of normalized_df. Because output prediction for the first 30 days (first time slice) doesn't exist.\n",
    "print(predicted_close_value.shape, real_close_value.shape)\n",
    "real_close_value = np.array(real_close_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2503, 1)\n"
     ]
    }
   ],
   "source": [
    "def get_pred(day_index, model, label, time_step, num_features, normalizer):\n",
    "    if day_index>len(label):\n",
    "        print(\"Day:\",day_index,\"is out of series range:\",len(label))\n",
    "        return\n",
    "    \n",
    "    sequence = np.array([label[i][0] for i in range(day_index-time_step,day_index)])\n",
    "    print(sequence.shape)\n",
    "    sequence = np.reshape(sequence, (1, time_step, num_features))\n",
    "    pred = model.predict(sequence)\n",
    "    pred = normalizer.inverse_transform(pd.DataFrame(pred))\n",
    "    x_coords = [i for i in range(day_index-time_step,day_index)]\n",
    "    y_coords = [label[i][0] for i in range(day_index-time_step,day_index)]\n",
    "    x_on_day = [day_index-1,day_index]\n",
    "    y_on_day = [label[day_index-1],pred[0][0]]\n",
    "    plt.plot(x_coords, y_coords, color = 'blue', label = 'Close')\n",
    "    plt.plot(x_on_day, y_on_day, color = 'red', label = 'Predicted Close')\n",
    "    plt.xticks([])\n",
    "    plt.title('Previous Month and Today')\n",
    "    plt.ylabel('Day')\n",
    "    plt.ylabel('Close')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "get_pred(2503, model, real_close_value, time_step, 1, normalizer)\n",
    "print(real_close_value.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lstmenv",
   "language": "python",
   "name": "lstmenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
