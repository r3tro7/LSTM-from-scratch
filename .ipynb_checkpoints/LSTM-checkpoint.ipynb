{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "#import project modules\n",
    "from modules import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dataset(stock_market_dataset):\n",
    "    normalizer = preprocessing.MinMaxScaler((0,1))\n",
    "    smd_exclude_strings = stock_market_dataset.select_dtypes(include = [np.number])\n",
    "    normalized = normalizer.fit_transform(smd_exclude_strings)\n",
    "    normalized_df = pd.DataFrame(normalized) \n",
    "    return [normalized, normalized_df, normalizer]\n",
    "\n",
    "def denormalize_data(df, normalizer, time_step, predictions, close_value_col_index):\n",
    "    df_copy = df.copy(deep = True)\n",
    "    rows_to_drop = [i for i in range(0,time_step)]\n",
    "    df_copy.drop(rows_to_drop,inplace = True)     \n",
    "    #deleting first #time_step rows of normalized_df. Because output prediction for the first 30 days (first time slice) doesn't exist.\n",
    "\n",
    "    #converting prediction array to dataframe for replacing actual row with prediction row\n",
    "    predicted_normalised_values_df = pd.DataFrame(predictions) \n",
    "\n",
    "    #replacing\n",
    "    df_copy[close_value_col_index] = predicted_normalised_values_df[0].values\n",
    "    \n",
    "    temp = normalizer.inverse_transform(df_copy)\n",
    "    df_copy = pd.DataFrame(temp)\n",
    "    return df_copy\n",
    "\n",
    "def make_train_test_val_sets(dataarray, num_features, close_value_col_index, time_step, train_per, val_per, test_per):\n",
    "    n = dataarray.shape[0] \n",
    "\n",
    "    #slices of data into time_steps \n",
    "    X_slice=[]\n",
    "    y_slice=[]\n",
    "    #normalized df dim 3125, 13\n",
    "    # i = 30 - 3125\n",
    "\n",
    "    for i in range(time_step, n):\n",
    "        X_slice.append(dataarray[ i-time_step:i ,  0:num_features ])      #1 example having data from 30 days dim 30x13\n",
    "        y_slice.append(dataarray[i,close_value_col_index])                  #close value data of the 31st day \n",
    "\n",
    "    #splitting percentage\n",
    "\n",
    "    #splitting slices for test,val,train and converting into np array\n",
    "    X_train=np.array( X_slice[ 0:int(n*train_per) ])\n",
    "    y_train=np.array( y_slice[ 0:int(n*train_per) ])\n",
    "\n",
    "    X_val=np.array( X_slice[ int(n*train_per):int(n*(train_per+val_per)) ])\n",
    "    y_val=np.array( y_slice[ int(n*train_per):int(n*(train_per+val_per)) ])\n",
    "\n",
    "    X_test=np.array( X_slice[ int(n*(train_per+val_per)): ])\n",
    "    y_test=np.array( y_slice[ int(n*(train_per+val_per)): ])\n",
    "    \n",
    "    print(np.array(X_train).shape)\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], num_features))    \n",
    "    #dimension of X_train 3095, 30, 13\n",
    "    #dimension of y_train  \n",
    "    print(np.array(X_train).shape)\n",
    "    return [X_train, y_train, X_val, y_val, X_test, y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2187, 30, 6)\n",
      "(2187, 30, 6)\n",
      "(2187, 30, 6) training dataset shape\n",
      "(625, 30, 6)  val dataset shape\n",
      "(283, 30, 6)  test dataset shape\n",
      "(2187,)\n"
     ]
    }
   ],
   "source": [
    "data_set_file = 'Google.csv'\n",
    "stock_market_dataset = pd.read_csv(data_set_file)\n",
    "stock_market_dataset.shape\n",
    "stock_market_dataset.head(3125)\n",
    "stock_market_dataset.columns.tolist()\n",
    "\n",
    "###normalize data\n",
    "normalized_array, normalized_df, normalizer = normalize_dataset(stock_market_dataset)\n",
    "normalized_df.head()\n",
    "n = normalized_df.shape[0]\n",
    "\n",
    "###split data set into train, cross validation and test set\n",
    "\n",
    "num_features = 6 \n",
    "close_value_col_index = 3\n",
    "#creating training set with time steps.\n",
    "\n",
    "time_step=30    #1 month time step\n",
    "train_per = 0.7\n",
    "val_per = 0.2\n",
    "test_per = 1-(train_per+val_per)  #0.1\n",
    "    \n",
    "X_train, y_train, X_val, y_val, X_test, y_test =  make_train_test_val_sets(normalized_array, num_features, close_value_col_index, time_step, train_per, val_per, test_per)\n",
    "\n",
    "print(str(X_train.shape)+ \" training dataset shape\")\n",
    "print(str(X_val.shape)+ \"  val dataset shape\")\n",
    "print(str(X_test.shape)+ \"  test dataset shape\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_arr(a, b, *args): \n",
    "    np.random.seed(0)\n",
    "    return np.random.rand(*args) * (b - a) + a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmParam:\n",
    "    def __init__(self, ct_dim, x_dim):\n",
    "        \"\"\"\n",
    "        \n",
    "        initialize all weights and biases \n",
    "        ct_dim - the dimension of current cell state (Ct) matrix\n",
    "        x_dim - the dimension of Tth input\n",
    "        \n",
    "        Weight and bias matrices are initialized with random values instead of zeroes to add noise. \n",
    "        Their derivatives will thus be zero.\n",
    "        \n",
    "        Terminology:\n",
    "        \n",
    "        Prefixes\n",
    "        w - a weight matrix\n",
    "        b - a bias matrix\n",
    "        d - a derivative matrix\n",
    "        \n",
    "        Suffixes\n",
    "        c - cell state gate. Represents data held in current cell. \n",
    "            It is the c't (c bar t) matrix\n",
    "        i - input gate\n",
    "        f - forget gate\n",
    "        o - output gate        \n",
    "        \n",
    "        \"\"\"\n",
    "        self.ct_dim = ct_dim  \n",
    "        self.x_dim = x_dim \n",
    "        combined_dim = x_dim + ct_dim\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.wc = rand_arr(-0.1, 0.1, ct_dim, combined_dim)\n",
    "        self.wi = rand_arr(-0.1, 0.1, ct_dim, combined_dim) \n",
    "        self.wf = rand_arr(-0.1, 0.1, ct_dim, combined_dim)\n",
    "        self.wo = rand_arr(-0.1, 0.1, ct_dim, combined_dim)\n",
    "        \n",
    "        #Initialize biases \n",
    "        self.bc = rand_arr(-0.1, 0.1, ct_dim) \n",
    "        self.bi = rand_arr(-0.1, 0.1, ct_dim) \n",
    "        self.bf = rand_arr(-0.1, 0.1, ct_dim) \n",
    "        self.bo = rand_arr(-0.1, 0.1, ct_dim) \n",
    "        \n",
    "        # Initialize derivatives\n",
    "        self.dwc = np.zeros((ct_dim, combined_dim)) \n",
    "        self.dwi = np.zeros((ct_dim, combined_dim)) \n",
    "        self.dwf = np.zeros((ct_dim, combined_dim)) \n",
    "        self.dwo = np.zeros((ct_dim, combined_dim)) \n",
    "        self.dbc = np.zeros(ct_dim) \n",
    "        self.dbi = np.zeros(ct_dim) \n",
    "        self.dbf = np.zeros(ct_dim) \n",
    "        self.dbo = np.zeros(ct_dim)\n",
    "        \n",
    "    def apply_derivatives(self, alpha = 1):\n",
    "        \"\"\"\n",
    "        Update parameters in each iteration to reach optimal value \n",
    "        alpha is the learning rate.\n",
    "        \"\"\"\n",
    "        self.wc -= alpha * self.dwc\n",
    "        self.wi -= alpha * self.dwi\n",
    "        self.wf -= alpha * self.dwf\n",
    "        self.wo -= alpha * self.dwo\n",
    "        self.bc -= alpha * self.dbc\n",
    "        self.bi -= alpha * self.dbi\n",
    "        self.bf -= alpha * self.dbf\n",
    "        self.bo -= alpha * self.dbo\n",
    "        \n",
    "        # reset all derivatives to zero\n",
    "        self.dwc = np.zeros_like(self.wc)\n",
    "        self.dwi = np.zeros_like(self.wi) \n",
    "        self.dwf = np.zeros_like(self.wf) \n",
    "        self.dwo = np.zeros_like(self.wo) \n",
    "        self.dbc = np.zeros_like(self.bc)\n",
    "        self.dbi = np.zeros_like(self.bi) \n",
    "        self.dbf = np.zeros_like(self.bf) \n",
    "        self.dbo = np.zeros_like(self.bo) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmCellState:\n",
    "    def __init__(self, ct_dim, x_dim):\n",
    "        \"\"\"\n",
    "        Initialize all gate matrices. \n",
    "        All gate matrices have the same dimension as ct matrix\n",
    "        c - current hidden cell state. The gate corresponding to this determines how much \n",
    "            data of previous cell should be read in current cell.\n",
    "        i - input gate. Determines how much data should be read into the cell from current input.\n",
    "        f - forget gate. Determines how much data should be forgotten, i.e discarded\n",
    "        o - output gate. How much data to output from current cell to next cell\n",
    "        s - The present state of gate. \n",
    "            Equation to calculate present state : forget_gate*previous_state(s<t-1>) + c_gate*input_gate\n",
    "        h - output state of the cell. It is the prediction value of Tth output in series.\n",
    "        dh - derivative of h\n",
    "        ds - derivative of s\n",
    "        \"\"\"\n",
    "        self.c = np.zeros(ct_dim)\n",
    "        self.i = np.zeros(ct_dim)\n",
    "        self.f = np.zeros(ct_dim)\n",
    "        self.o = np.zeros(ct_dim)\n",
    "        self.s = np.zeros(ct_dim)\n",
    "        self.h = np.zeros(ct_dim)\n",
    "        self.dh = np.zeros_like(self.h)\n",
    "        self.ds = np.zeros_like(self.s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmLayer:\n",
    "    def __init__(self, lstm_param, lstm_cell_state):\n",
    "        self.state = lstm_cell_state\n",
    "        self.param = lstm_param\n",
    "        #xh is the concatenation ofprevious layer's output with current input.\n",
    "        self.xh = None\n",
    "\n",
    "    def calculate_gates(self, x, s_prev = None, h_prev = None):\n",
    "        \"\"\"\n",
    "        if this is the first lstm layer in the network then \n",
    "        s_prev and h_prev will be initialized to zero as cell state \n",
    "        and output states are not present.\n",
    "        \n",
    "        s_prev = cell state of previous layer's cells.\n",
    "        h_prev = output state from previous layer\n",
    "        \n",
    "        \"\"\"\n",
    "        if s_prev is None: \n",
    "            s_prev = np.zeros_like(self.state.s)\n",
    "        if h_prev is None: \n",
    "            h_prev = np.zeros_like(self.state.h)\n",
    "        \n",
    "        # save previous states for use in backprop\n",
    "        self.s_prev = s_prev\n",
    "        self.h_prev = h_prev\n",
    "\n",
    "        # concatenate x(T) and h(T-1)\n",
    "        xh = np.hstack((x,  h_prev))\n",
    "        #Calculate gate values\n",
    "        self.state.c = tanh(np.dot(self.param.wc, xh) + self.param.bc)\n",
    "        self.state.i = sigmoid(np.dot(self.param.wi, xh) + self.param.bi)\n",
    "        self.state.f = sigmoid(np.dot(self.param.wf, xh) + self.param.bf)\n",
    "        self.state.o = sigmoid(np.dot(self.param.wo, xh) + self.param.bo)\n",
    "        self.state.s = self.state.c * self.state.i + s_prev * self.state.f\n",
    "        self.state.h = self.state.s * self.state.o\n",
    "        self.xh = xh\n",
    "    \n",
    "    def calculate_derivatives(self, dh, ds):\n",
    "        ds = self.state.o * dh + ds\n",
    "        do = self.state.s * dh\n",
    "        di = self.state.c * ds\n",
    "        dc = self.state.i * ds\n",
    "        df = self.s_prev * ds\n",
    "\n",
    "        # calculate derivatives w.r.t. gate inside sigma / tanh function\n",
    "        di_input = dsigmoid(self.state.i) * di \n",
    "        df_input = dsigmoid(self.state.f) * df \n",
    "        do_input = dsigmoid(self.state.o) * do \n",
    "        dc_input = dtanh(self.state.c) * dc\n",
    "\n",
    "        # derivatives w.r.t. inputs\n",
    "        self.param.dwc += np.outer(dc_input, self.xh)\n",
    "        self.param.dwi += np.outer(di_input, self.xh)\n",
    "        self.param.dwf += np.outer(df_input, self.xh)\n",
    "        self.param.dwo += np.outer(do_input, self.xh)\n",
    "        self.param.dbc += dc_input       \n",
    "        self.param.dbi += di_input\n",
    "        self.param.dbf += df_input       \n",
    "        self.param.dbo += do_input\n",
    "        \n",
    "        # calculate derivative for xh\n",
    "        dxh = np.zeros_like(self.xh)\n",
    "        dxh += np.dot(self.param.wc.T, dc_input)\n",
    "        dxh += np.dot(self.param.wi.T, di_input)\n",
    "        dxh += np.dot(self.param.wf.T, df_input)\n",
    "        dxh += np.dot(self.param.wo.T, do_input)\n",
    "        \n",
    "        # save derivatives\n",
    "        self.state.ds = ds * self.state.f\n",
    "        self.state.dh = dxh[self.param.x_dim:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmNetwork():\n",
    "    def __init__(self, lstm_param):\n",
    "        \"\"\"\n",
    "        x_list - the sequence that'll be iput to lstm\n",
    "        lstm_layer_list - the ouput from layer that will be input to next layer\n",
    "        \"\"\"\n",
    "        self.lstm_param = lstm_param\n",
    "        self.lstm_layer_list = []\n",
    "        self.x_list = []\n",
    "\n",
    "    def get_loss(self, y_list, loss_layer):\n",
    "        \"\"\"\n",
    "        Updates derivatives w.r.t corresponding loss layer. \n",
    "        To update parameters, we will call self.lstm_param.apply_derivatives()\n",
    "        \"\"\"\n",
    "        assert len(y_list) == len(self.x_list)\n",
    "        index = len(self.x_list) - 1\n",
    "        # Calculate loss for the last layer \n",
    "        loss = loss_layer.loss(self.lstm_layer_list[index].state.h, y_list[index])\n",
    "        diff_h = loss_layer.derivative(self.lstm_layer_list[index].state.h, y_list[index])\n",
    "        # For the last layer of the network, diff_s will be\n",
    "        diff_s = np.zeros(self.lstm_param.ct_dim)\n",
    "        self.lstm_layer_list[index].calculate_derivatives(diff_h, diff_s)\n",
    "        index -= 1\n",
    "\n",
    "        while index >= 0:\n",
    "            loss += loss_layer.loss(self.lstm_layer_list[index].state.h, y_list[index])\n",
    "            diff_h = loss_layer.derivative(self.lstm_layer_list[index].state.h, y_list[index])\n",
    "            diff_h += self.lstm_layer_list[index + 1].state.dh\n",
    "            diff_s = self.lstm_layer_list[index + 1].state.ds\n",
    "            self.lstm_layer_list[index].calculate_derivatives(diff_h, diff_s)\n",
    "            index -= 1 \n",
    "\n",
    "        return loss\n",
    "\n",
    "    def clear_x_list(self):\n",
    "        self.x_list = []\n",
    "\n",
    "    def add_x_list(self, x):\n",
    "        self.x_list.append(x)\n",
    "        if len(self.x_list) > len(self.lstm_layer_list):\n",
    "            lstm_state = LstmCellState(self.lstm_param.ct_dim, self.lstm_param.x_dim)\n",
    "            self.lstm_layer_list.append(LstmLayer(self.lstm_param, lstm_state))\n",
    "\n",
    "        # get index of most recent x input\n",
    "        index = len(self.x_list) - 1\n",
    "        if index == 0:\n",
    "            self.lstm_layer_list[index].calculate_gates(x)\n",
    "        else:\n",
    "            s_prev = self.lstm_layer_list[index - 1].state.s\n",
    "            h_prev = self.lstm_layer_list[index - 1].state.h\n",
    "            self.lstm_layer_list[index].calculate_gates(x, s_prev, h_prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter  0: y_pred = [ 0.04135,  0.06930,  0.11699,  0.16562], loss: 1.703e+00\n",
      "iter  1: y_pred = [-0.21158, -0.30790, -0.36603, -0.37193], loss: 9.926e-01\n",
      "iter  2: y_pred = [-0.22250, -0.29354, -0.36030, -0.41698], loss: 9.168e-01\n",
      "iter  3: y_pred = [-0.23755, -0.28552, -0.36098, -0.47106], loss: 8.461e-01\n",
      "iter  4: y_pred = [-0.25128, -0.27542, -0.35710, -0.51905], loss: 7.814e-01\n",
      "iter  5: y_pred = [-0.26383, -0.26362, -0.34902, -0.55952], loss: 7.227e-01\n",
      "iter  6: y_pred = [-0.27620, -0.25156, -0.33880, -0.59443], loss: 6.694e-01\n",
      "iter  7: y_pred = [-0.28887, -0.23983, -0.32745, -0.62491], loss: 6.209e-01\n",
      "iter  8: y_pred = [-0.30201, -0.22865, -0.31557, -0.65164], loss: 5.765e-01\n",
      "iter  9: y_pred = [-0.31565, -0.21809, -0.30353, -0.67514], loss: 5.359e-01\n",
      "iter 10: y_pred = [-0.32974, -0.20817, -0.29158, -0.69580], loss: 4.987e-01\n",
      "iter 11: y_pred = [-0.34416, -0.19887, -0.27989, -0.71394], loss: 4.647e-01\n",
      "iter 12: y_pred = [-0.35879, -0.19014, -0.26858, -0.72984], loss: 4.334e-01\n",
      "iter 13: y_pred = [-0.37347, -0.18195, -0.25772, -0.74374], loss: 4.049e-01\n",
      "iter 14: y_pred = [-0.38805, -0.17425, -0.24735, -0.75590], loss: 3.788e-01\n",
      "iter 15: y_pred = [-0.40240, -0.16701, -0.23750, -0.76653], loss: 3.550e-01\n",
      "iter 16: y_pred = [-0.41640, -0.16019, -0.22817, -0.77584], loss: 3.333e-01\n",
      "iter 17: y_pred = [-0.42996, -0.15376, -0.21935, -0.78401], loss: 3.135e-01\n",
      "iter 18: y_pred = [-0.44300, -0.14769, -0.21102, -0.79120], loss: 2.955e-01\n",
      "iter 19: y_pred = [-0.45548, -0.14197, -0.20317, -0.79756], loss: 2.791e-01\n",
      "iter 20: y_pred = [-0.46735, -0.13656, -0.19577, -0.80322], loss: 2.642e-01\n",
      "iter 21: y_pred = [-0.47861, -0.13145, -0.18879, -0.80828], loss: 2.507e-01\n",
      "iter 22: y_pred = [-0.48926, -0.12661, -0.18222, -0.81283], loss: 2.383e-01\n",
      "iter 23: y_pred = [-0.49930, -0.12204, -0.17601, -0.81695], loss: 2.271e-01\n",
      "iter 24: y_pred = [-0.50875, -0.11772, -0.17016, -0.82069], loss: 2.168e-01\n",
      "iter 25: y_pred = [-0.51763, -0.11362, -0.16464, -0.82412], loss: 2.074e-01\n",
      "iter 26: y_pred = [-0.52598, -0.10974, -0.15942, -0.82726], loss: 1.988e-01\n",
      "iter 27: y_pred = [-0.53381, -0.10607, -0.15448, -0.83017], loss: 1.909e-01\n",
      "iter 28: y_pred = [-0.54117, -0.10258, -0.14981, -0.83286], loss: 1.837e-01\n",
      "iter 29: y_pred = [-0.54807, -0.09928, -0.14538, -0.83537], loss: 1.770e-01\n",
      "iter 30: y_pred = [-0.55455, -0.09614, -0.14118, -0.83771], loss: 1.709e-01\n",
      "iter 31: y_pred = [-0.56064, -0.09316, -0.13720, -0.83989], loss: 1.652e-01\n",
      "iter 32: y_pred = [-0.56636, -0.09032, -0.13342, -0.84195], loss: 1.600e-01\n",
      "iter 33: y_pred = [-0.57174, -0.08763, -0.12982, -0.84388], loss: 1.551e-01\n",
      "iter 34: y_pred = [-0.57681, -0.08507, -0.12640, -0.84570], loss: 1.506e-01\n",
      "iter 35: y_pred = [-0.58158, -0.08262, -0.12314, -0.84741], loss: 1.465e-01\n",
      "iter 36: y_pred = [-0.58608, -0.08030, -0.12003, -0.84903], loss: 1.426e-01\n",
      "iter 37: y_pred = [-0.59032, -0.07808, -0.11707, -0.85057], loss: 1.389e-01\n",
      "iter 38: y_pred = [-0.59432, -0.07596, -0.11424, -0.85202], loss: 1.355e-01\n",
      "iter 39: y_pred = [-0.59811, -0.07394, -0.11153, -0.85340], loss: 1.323e-01\n",
      "iter 40: y_pred = [-0.60169, -0.07201, -0.10895, -0.85471], loss: 1.294e-01\n",
      "iter 41: y_pred = [-0.60508, -0.07016, -0.10647, -0.85596], loss: 1.266e-01\n",
      "iter 42: y_pred = [-0.60829, -0.06839, -0.10410, -0.85715], loss: 1.239e-01\n",
      "iter 43: y_pred = [-0.61134, -0.06670, -0.10183, -0.85828], loss: 1.215e-01\n",
      "iter 44: y_pred = [-0.61423, -0.06508, -0.09965, -0.85936], loss: 1.191e-01\n",
      "iter 45: y_pred = [-0.61698, -0.06353, -0.09756, -0.86039], loss: 1.169e-01\n",
      "iter 46: y_pred = [-0.61959, -0.06204, -0.09555, -0.86137], loss: 1.149e-01\n",
      "iter 47: y_pred = [-0.62208, -0.06061, -0.09362, -0.86231], loss: 1.129e-01\n",
      "iter 48: y_pred = [-0.62444, -0.05923, -0.09176, -0.86321], loss: 1.110e-01\n",
      "iter 49: y_pred = [-0.62670, -0.05792, -0.08997, -0.86407], loss: 1.093e-01\n",
      "iter 50: y_pred = [-0.62885, -0.05665, -0.08825, -0.86489], loss: 1.076e-01\n",
      "iter 51: y_pred = [-0.63090, -0.05543, -0.08659, -0.86569], loss: 1.060e-01\n",
      "iter 52: y_pred = [-0.63286, -0.05425, -0.08499, -0.86644], loss: 1.045e-01\n",
      "iter 53: y_pred = [-0.63473, -0.05312, -0.08345, -0.86717], loss: 1.031e-01\n",
      "iter 54: y_pred = [-0.63652, -0.05203, -0.08196, -0.86787], loss: 1.017e-01\n",
      "iter 55: y_pred = [-0.63824, -0.05098, -0.08052, -0.86855], loss: 1.004e-01\n",
      "iter 56: y_pred = [-0.63988, -0.04997, -0.07913, -0.86919], loss: 9.913e-02\n",
      "iter 57: y_pred = [-0.64145, -0.04899, -0.07778, -0.86982], loss: 9.794e-02\n",
      "iter 58: y_pred = [-0.64296, -0.04805, -0.07649, -0.87042], loss: 9.680e-02\n",
      "iter 59: y_pred = [-0.64440, -0.04713, -0.07523, -0.87100], loss: 9.571e-02\n",
      "iter 60: y_pred = [-0.64579, -0.04625, -0.07401, -0.87155], loss: 9.467e-02\n",
      "iter 61: y_pred = [-0.64712, -0.04540, -0.07283, -0.87209], loss: 9.367e-02\n",
      "iter 62: y_pred = [-0.64841, -0.04458, -0.07169, -0.87261], loss: 9.271e-02\n",
      "iter 63: y_pred = [-0.64964, -0.04378, -0.07058, -0.87311], loss: 9.179e-02\n",
      "iter 64: y_pred = [-0.65082, -0.04301, -0.06951, -0.87360], loss: 9.090e-02\n",
      "iter 65: y_pred = [-0.65196, -0.04226, -0.06846, -0.87406], loss: 9.005e-02\n",
      "iter 66: y_pred = [-0.65306, -0.04154, -0.06745, -0.87452], loss: 8.923e-02\n",
      "iter 67: y_pred = [-0.65411, -0.04083, -0.06647, -0.87495], loss: 8.844e-02\n",
      "iter 68: y_pred = [-0.65513, -0.04015, -0.06551, -0.87538], loss: 8.769e-02\n",
      "iter 69: y_pred = [-0.65611, -0.03949, -0.06458, -0.87579], loss: 8.696e-02\n",
      "iter 70: y_pred = [-0.65706, -0.03885, -0.06368, -0.87619], loss: 8.625e-02\n",
      "iter 71: y_pred = [-0.65797, -0.03823, -0.06280, -0.87657], loss: 8.557e-02\n",
      "iter 72: y_pred = [-0.65886, -0.03762, -0.06195, -0.87695], loss: 8.492e-02\n",
      "iter 73: y_pred = [-0.65971, -0.03704, -0.06112, -0.87731], loss: 8.428e-02\n",
      "iter 74: y_pred = [-0.66053, -0.03647, -0.06031, -0.87766], loss: 8.367e-02\n",
      "iter 75: y_pred = [-0.66133, -0.03591, -0.05952, -0.87800], loss: 8.308e-02\n",
      "iter 76: y_pred = [-0.66210, -0.03537, -0.05876, -0.87834], loss: 8.251e-02\n",
      "iter 77: y_pred = [-0.66284, -0.03485, -0.05801, -0.87866], loss: 8.196e-02\n",
      "iter 78: y_pred = [-0.66356, -0.03434, -0.05728, -0.87897], loss: 8.142e-02\n",
      "iter 79: y_pred = [-0.66426, -0.03384, -0.05657, -0.87928], loss: 8.090e-02\n",
      "iter 80: y_pred = [-0.66493, -0.03335, -0.05587, -0.87957], loss: 8.040e-02\n",
      "iter 81: y_pred = [-0.66558, -0.03288, -0.05520, -0.87986], loss: 7.991e-02\n",
      "iter 82: y_pred = [-0.66622, -0.03242, -0.05454, -0.88014], loss: 7.944e-02\n",
      "iter 83: y_pred = [-0.66683, -0.03197, -0.05389, -0.88042], loss: 7.898e-02\n",
      "iter 84: y_pred = [-0.66743, -0.03154, -0.05326, -0.88068], loss: 7.853e-02\n",
      "iter 85: y_pred = [-0.66801, -0.03111, -0.05265, -0.88094], loss: 7.810e-02\n",
      "iter 86: y_pred = [-0.66857, -0.03070, -0.05204, -0.88120], loss: 7.768e-02\n",
      "iter 87: y_pred = [-0.66911, -0.03029, -0.05146, -0.88144], loss: 7.727e-02\n",
      "iter 88: y_pred = [-0.66964, -0.02990, -0.05088, -0.88168], loss: 7.688e-02\n",
      "iter 89: y_pred = [-0.67015, -0.02951, -0.05032, -0.88192], loss: 7.649e-02\n",
      "iter 90: y_pred = [-0.67065, -0.02913, -0.04977, -0.88215], loss: 7.611e-02\n",
      "iter 91: y_pred = [-0.67113, -0.02877, -0.04924, -0.88237], loss: 7.575e-02\n",
      "iter 92: y_pred = [-0.67160, -0.02841, -0.04871, -0.88259], loss: 7.539e-02\n",
      "iter 93: y_pred = [-0.67206, -0.02806, -0.04819, -0.88280], loss: 7.505e-02\n",
      "iter 94: y_pred = [-0.67251, -0.02771, -0.04769, -0.88301], loss: 7.471e-02\n",
      "iter 95: y_pred = [-0.67294, -0.02738, -0.04720, -0.88321], loss: 7.438e-02\n",
      "iter 96: y_pred = [-0.67336, -0.02705, -0.04671, -0.88341], loss: 7.406e-02\n",
      "iter 97: y_pred = [-0.67377, -0.02673, -0.04624, -0.88361], loss: 7.375e-02\n",
      "iter 98: y_pred = [-0.67417, -0.02641, -0.04578, -0.88380], loss: 7.344e-02\n",
      "iter 99: y_pred = [-0.67456, -0.02611, -0.04532, -0.88398], loss: 7.315e-02\n"
     ]
    }
   ],
   "source": [
    "class LossLayer:\n",
    "    \"\"\"\n",
    "    Computes square loss with first element of hidden layer array.\n",
    "    \"\"\"\n",
    "    @classmethod\n",
    "    def loss(self, pred, label):\n",
    "        return (pred[0] - label) ** 2\n",
    "\n",
    "    @classmethod\n",
    "    def derivative(self, pred, label):\n",
    "        diff = np.zeros_like(pred)\n",
    "        diff[0] = 2 * (pred[0] - label)\n",
    "        return diff\n",
    "\n",
    "\n",
    "def example_0():\n",
    "    # learns to repeat simple sequence from random inputs\n",
    "    np.random.seed(0)\n",
    "\n",
    "    # parameters for input data dimension and lstm cell count\n",
    "    ct_dim = 100\n",
    "    x_dim = 50\n",
    "    lstm_param = LstmParam(ct_dim, x_dim)\n",
    "    lstm_net = LstmNetwork(lstm_param)\n",
    "    y_list = [-0.7, 0.2, 0.1, -0.9]\n",
    "    input_val_arr = [np.random.random(x_dim) for _ in y_list]\n",
    "    \n",
    "    for cur_iter in range(100):\n",
    "        print(\"iter\", \"%2s\" % str(cur_iter), end=\": \")\n",
    "        for ind in range(len(y_list)):\n",
    "            lstm_net.add_x_list(input_val_arr[ind])\n",
    "\n",
    "        print(\"y_pred = [\" +\n",
    "              \", \".join([\"% 2.5f\" % lstm_net.lstm_layer_list[ind].state.h[0] for ind in range(len(y_list))]) +\n",
    "              \"]\", end=\", \")\n",
    "\n",
    "        loss = lstm_net.get_loss(y_list, LossLayer)\n",
    "        print(\"loss:\", \"%.3e\" % loss)\n",
    "        lstm_param.apply_derivatives(0.1)\n",
    "        lstm_net.clear_x_list()\n",
    "\n",
    "\n",
    "\n",
    "example_0()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lstmenv",
   "language": "python",
   "name": "lstmenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
